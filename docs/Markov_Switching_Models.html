<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Markov-Switching-Models – Julian Herbert Müller</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-1740faba99ef51bb07737d25e066e4ec.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar docked nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">Julian Herbert Müller</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="./index.html"> 
<span class="menu-text">Start</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./AboutMe.html"> 
<span class="menu-text">About Me</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link active" href="./My_Oxford_Time.html" aria-current="page"> 
<span class="menu-text">Oxford Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./YouTube.html"> 
<span class="menu-text">YouTube</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/jmuelleo" target="_blank"> <i class="bi bi-github" role="img" aria-label="GitHub">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/julian-m%C3%BCller-6b9974250/" target="_blank"> <i class="bi bi-linkedin" role="img" aria-label="LinkedIn">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.instagram.com/julianmueller6416/" target="_blank"> <i class="bi bi-instagram" role="img" aria-label="Instagram">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.youtube.com/channel/UC-iICKVCNn7iEQicgRDTudw" target="_blank"> <i class="bi bi-youtube" role="img" aria-label="YouTube">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./Markov_Switching_Models.html">Theory Blog</a></li><li class="breadcrumb-item"><a href="./Markov_Switching_Models.html">Markov-Switching Models</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
 <span class="menu-text"><strong>Post Navigator</strong></span>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Theory Blog</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Markov_Switching_Models.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Markov-Switching Models</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false">
 <span class="menu-text">Oxford Blog</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./My_Oxford_Time.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">My time at Oxford University</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#brief-remarks-on-notation" id="toc-brief-remarks-on-notation" class="nav-link active" data-scroll-target="#brief-remarks-on-notation">Brief Remarks on Notation</a></li>
  <li><a href="#markov-chains-and-autoregressive-processes" id="toc-markov-chains-and-autoregressive-processes" class="nav-link" data-scroll-target="#markov-chains-and-autoregressive-processes">Markov-Chains and Autoregressive Processes</a>
  <ul class="collapse">
  <li><a href="#markov-chains" id="toc-markov-chains" class="nav-link" data-scroll-target="#markov-chains">Markov-Chains</a></li>
  <li><a href="#autoregressive-ar-processes" id="toc-autoregressive-ar-processes" class="nav-link" data-scroll-target="#autoregressive-ar-processes">Autoregressive (AR) Processes</a></li>
  <li><a href="#introduction-to-markov-switching-models" id="toc-introduction-to-markov-switching-models" class="nav-link" data-scroll-target="#introduction-to-markov-switching-models">Introduction to Markov-Switching Models</a></li>
  <li><a href="#optimal-inference-of-the-regimes-and-derivation-of-the-log-likelihood" id="toc-optimal-inference-of-the-regimes-and-derivation-of-the-log-likelihood" class="nav-link" data-scroll-target="#optimal-inference-of-the-regimes-and-derivation-of-the-log-likelihood">Optimal Inference of the Regimes and Derivation of the Log-Likelihood</a></li>
  <li><a href="#smoothed-inference-over-the-regimes" id="toc-smoothed-inference-over-the-regimes" class="nav-link" data-scroll-target="#smoothed-inference-over-the-regimes">Smoothed Inference over the Regimes</a></li>
  <li><a href="#optimisation-of-the-conditional-log-likelihood" id="toc-optimisation-of-the-conditional-log-likelihood" class="nav-link" data-scroll-target="#optimisation-of-the-conditional-log-likelihood">Optimisation of the Conditional Log-Likelihood</a>
  <ul class="collapse">
  <li><a href="#general-em-algorithm-theory" id="toc-general-em-algorithm-theory" class="nav-link" data-scroll-target="#general-em-algorithm-theory">General EM Algorithm Theory</a></li>
  <li><a href="#application-of-the-em-algorithm-to-markov-switching-ar-models" id="toc-application-of-the-em-algorithm-to-markov-switching-ar-models" class="nav-link" data-scroll-target="#application-of-the-em-algorithm-to-markov-switching-ar-models">Application of the EM Algorithm to Markov-Switching AR Models</a></li>
  <li><a href="#example-0-switching-coefficients-and-intercept-non-switching-sigma2" id="toc-example-0-switching-coefficients-and-intercept-non-switching-sigma2" class="nav-link" data-scroll-target="#example-0-switching-coefficients-and-intercept-non-switching-sigma2">Example 0: Switching Coefficients and Intercept, Non-Switching <span class="math inline">\(\sigma^2\)</span></a></li>
  <li><a href="#example-1-non-switching-intercept" id="toc-example-1-non-switching-intercept" class="nav-link" data-scroll-target="#example-1-non-switching-intercept">Example 1: Non-Switching Intercept</a></li>
  <li><a href="#example-2-switching-intercept-and-non-switching-coefficients" id="toc-example-2-switching-intercept-and-non-switching-coefficients" class="nav-link" data-scroll-target="#example-2-switching-intercept-and-non-switching-coefficients">Example 2: Switching Intercept and Non-Switching Coefficients</a></li>
  <li><a href="#example-3-all-parameters-switch" id="toc-example-3-all-parameters-switch" class="nav-link" data-scroll-target="#example-3-all-parameters-switch">Example 3: All Parameters Switch</a></li>
  <li><a href="#example-4-arbitrary-subset-switching-of-cphi-and-non-switching-sigma2" id="toc-example-4-arbitrary-subset-switching-of-cphi-and-non-switching-sigma2" class="nav-link" data-scroll-target="#example-4-arbitrary-subset-switching-of-cphi-and-non-switching-sigma2">Example 4: Arbitrary Subset-Switching of <span class="math inline">\((c,\phi)\)</span> and Non-Switching <span class="math inline">\(\sigma^2\)</span></a></li>
  <li><a href="#example-5-arbitrary-subset-switching-of-cphi-and-switching-sigma2" id="toc-example-5-arbitrary-subset-switching-of-cphi-and-switching-sigma2" class="nav-link" data-scroll-target="#example-5-arbitrary-subset-switching-of-cphi-and-switching-sigma2">Example 5: Arbitrary Subset-Switching of <span class="math inline">\((c,\phi)\)</span> and Switching <span class="math inline">\(\sigma^2\)</span></a></li>
  </ul></li>
  <li><a href="#forecasting-with-markov-switching-models" id="toc-forecasting-with-markov-switching-models" class="nav-link" data-scroll-target="#forecasting-with-markov-switching-models">Forecasting with Markov-Switching Models</a></li>
  <li><a href="#regime-forecasting-with-markov-switching-models" id="toc-regime-forecasting-with-markov-switching-models" class="nav-link" data-scroll-target="#regime-forecasting-with-markov-switching-models">Regime Forecasting with Markov-Switching Models</a></li>
  </ul></li>
  <li><a href="#appendix" id="toc-appendix" class="nav-link" data-scroll-target="#appendix">Appendix</a>
  <ul class="collapse">
  <li><a href="#optimal-inference-of-the-regimes-and-derivation-of-the-log-likelihood-1" id="toc-optimal-inference-of-the-regimes-and-derivation-of-the-log-likelihood-1" class="nav-link" data-scroll-target="#optimal-inference-of-the-regimes-and-derivation-of-the-log-likelihood-1">Optimal Inference of the Regimes and Derivation of the Log-Likelihood</a></li>
  <li><a href="#smoothed-inference-over-the-regimes-1" id="toc-smoothed-inference-over-the-regimes-1" class="nav-link" data-scroll-target="#smoothed-inference-over-the-regimes-1">Smoothed Inference over the Regimes</a></li>
  <li><a href="#em-algorithm-for-autoregressive-processes-with-finite-lag-order" id="toc-em-algorithm-for-autoregressive-processes-with-finite-lag-order" class="nav-link" data-scroll-target="#em-algorithm-for-autoregressive-processes-with-finite-lag-order">EM Algorithm for Autoregressive Processes with finite lag order</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./Markov_Switching_Models.html">Theory Blog</a></li><li class="breadcrumb-item"><a href="./Markov_Switching_Models.html">Markov-Switching Models</a></li></ol></nav>
<div class="quarto-title">
<h1 class="title">Markov-Switching-Models</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>In this blogpost I will discuss the underlying theory behind Markov-Switching models, this includes a short review of Markov-Chains, as well as an in-depth derivation of the Hamilton-Filter, the Kim-Algorithm (for smoothed inference) and the EM-Algorithm. The derivations and explanations are mostly taken from my Bachelor Thesis, which discussed these topics as well. The equation numbers are the equation numbers as they are in Bachelor Thesis, so you can find them easily in the original work. My thesis can be downloaded at the end of my “About Me” section and additionally discusses the development of my R package (MSARM) and how MSARM outperforms MSwM regarding estimation robustness.</p>
<section id="brief-remarks-on-notation" class="level1">
<h1>Brief Remarks on Notation</h1>
<p>We start with a short discussion of the notation utilized in this paper. The density function of a continuous random variable <span class="math inline">\(Y\)</span> will be denoted as <span class="math inline">\(f_Y(y)\)</span>, the conditional density function, based on another random variable <span class="math inline">\(X\)</span> will be denoted as <span class="math inline">\(f_{Y|X}(y|x)\)</span>. We want to emphasize that in this notation, the subscript denotes the arguments of the density function, and the values in the parentheses denote the point at which the density function is evaluated. Furthermore we define a generalized density function for a vector of a discrete random variable <span class="math inline">\(S\)</span> and a continuous random variable <span class="math inline">\(Y\)</span> as <span class="math inline">\(f_{Y,S}(y,s) = f_{Y|S}(y|s)\cdot P(S = s)\)</span>. Last but not least it should be noted that we will denote a parameter vector <span class="math inline">\(\theta\)</span> that influences the distribution of a random variable with a semicolon after the arguments in the density function’s subscript. For probability functions, the parameter vector will also be denoted with a subscript. For example, for a continuous random variable <span class="math inline">\(Y\)</span>, we could write <span class="math inline">\(f_{Y;\theta}(y)\)</span> or for a discrete random variable <span class="math inline">\(S\)</span>, <span class="math inline">\(P_{\theta}(S = s)\)</span>. It should be noted that both could be seen as functions in <span class="math inline">\(\theta\)</span>.</p>
</section>
<section id="markov-chains-and-autoregressive-processes" class="level1">
<h1>Markov-Chains and Autoregressive Processes</h1>
<section id="markov-chains" class="level2">
<h2 class="anchored" data-anchor-id="markov-chains">Markov-Chains</h2>
<p>With the general notation out of the way, we can start to build the foundation of Markov-Switching AR models. For that, we start with Markov-Chains in general. Markov-Chains are stochastic processes satisfying the so-called Markov property. More precisely, we consider a sequence of random variables <span class="math inline">\(S_t, S_{t-1},...\)</span> that can take values in the set <span class="math inline">\(\{1, \dots, N\}\)</span>, and fulfill the property: <span class="math display">\[\begin{equation}
    P(S_t = j \mid S_{t-1} = i, \dots, S_1 = a_1) = P(S_t = j \mid S_{t-1} = i).
\end{equation}\]</span> This means that the next state is conditionally independent of all previous states given the current state. The transition probabilities between the states of such a Markov-Chain are usually summarized in a so-called transition matrix <span class="math inline">\(\Pi\)</span>, where the ith row and jth column element of <span class="math inline">\(\Pi\)</span> is given by <span class="math inline">\((\Pi){i,j} = \pi{i,j} = P(S_t = j \mid S_{t-1} = i)\)</span>. Thereby <span class="math inline">\((\cdot)_{i,j}\)</span> indicates the ith row and jth column element. Additionally, we want to point out that we will denote the transpose of a matrix <span class="math inline">\(A\)</span> as <span class="math inline">\(A'\)</span> throughout this thesis. Therefore, <span class="math inline">\(\Pi\)</span> is defined such that the rows indicate the previous state and the columns represent the state being transitioned into. Furthermore, the sum of the row elements must equal 1. It has to be noted that we can represent a Markov-Chain as a Vector-Autoregressive-Process (VAR). The following discussion of representing Markov-Chains as a VAR closely follows Hamilton (1994, page 678-680). Suppose we have an underlying Markov-Chain <span class="math inline">\(S_t, S_{t-1},...\)</span>, and the state space is <span class="math inline">\(\{1, \dots, N\}\)</span>. We then define: <span class="math display">\[\begin{equation}
\zeta_{t} =
\begin{cases}
(1,0,...,0)', &amp; \text{if } S_t = 1\\
(0,1,...,0)', &amp; \text{if } S_t = 2\\
...\\
(0,0,...,1)', &amp; \text{if } S_t = N\\
\end{cases}.
\end{equation}\]</span> Thus for <span class="math inline">\(S_t = i\)</span>, <span class="math inline">\(\zeta_{t}\)</span> equals the ith column of <span class="math inline">\(I_N\)</span>. If <span class="math inline">\(S_t = i\)</span>, then the jth element of <span class="math inline">\(\zeta_{t+1}\)</span> is a random variable with <span class="math inline">\(P((\zeta_{t+1})_j = 1|S_t = i) = \pi_{i,j}\)</span>. Thus <span class="math display">\[\begin{equation}
E(\zeta_{t+1}|S_t = i) =
\begin{pmatrix}
\pi_{i,1}\\
\pi_{i,2}\\
...\\
\pi_{i,N}
\end{pmatrix}.
\end{equation}\]</span> Furthermore, one should note that <span class="math inline">\(E(\zeta_{t+1}|S_t = i)\)</span> is the ith column of <span class="math inline">\(\Pi'\)</span>. Knowing for <span class="math inline">\(S_t = i\)</span>, <span class="math inline">\(\zeta_{t}\)</span> is equal to the ith column of <span class="math inline">\(I_N\)</span> it follows that <span class="math inline">\(E(\zeta_{t+1}|\zeta_{t}) = \Pi'\zeta_{t}\)</span>. Due to (1) it holds that <span class="math inline">\(E(\zeta_{t+1}|\zeta_{t},\zeta_{t-1},...) = E(\zeta_{t+1}|\zeta_{t}) = \Pi'\zeta_{t}\)</span>, therefore we can write: <span class="math display">\[\begin{equation}
\zeta_{t+1} = \Pi'\zeta_{t} + v_{t+1}; \quad \text{where} \quad v_{t+1} = \zeta_{t+1} - E(\zeta_{t+1}|\zeta_{t},\zeta_{t-1},...),
\end{equation}\]</span> (4) implicates that: <span class="math display">\[\begin{equation}
    \zeta_{t+m} = (\Pi')^{m}\zeta_t + (\Pi')^{m-1}v_{t+1} + ... + \Pi'v_{t+m-1} + v_{t+m}.
\end{equation}\]</span> This is due to the following derivation: <span class="math display">\[\begin{align*}
\zeta_{t+m} &amp;= \Pi'\zeta_{t+m-1} + v_{t+m}
\\ &amp;= \Pi'(\Pi'\zeta_{t+m-2} + v_{t+m-1}) + v_{t+m}
\\ &amp;= \Pi'(\Pi'(\Pi'\zeta_{t+m-3} + v_{t+m-2}) + v_{t+m-1}) + v_{t+m}
\\ &amp;= ...
\\ &amp;= (\Pi')^{m}\zeta_t + (\Pi')^{m-1}v_{t+1} + ... + \Pi'v_{t+m-1} + v_{t+m}.
\end{align*}\]</span></p>
<p>An m-period ahead forecast for a Markov-Chain can therefore be constructed in the following way: <span class="math display">\[\begin{equation}
    E(\zeta_{t+m}|\zeta_t,\zeta_{t-1},...) = (\Pi')^{m}\zeta_t.
\end{equation}\]</span> This holds because: <span class="math display">\[\begin{align*}
E(\zeta_{t+m}|\zeta_{t},\zeta_{t-1},...) &amp;= E(\Pi'\zeta_{t+m-1} + v_{t+m}|\zeta_{t},\zeta_{t-1},...)
\\ &amp;= E((\Pi')^{m}\zeta_t + (\Pi')^{m-1}v_{t+1} + ... + \Pi'v_{t+m-1} + v_{t+m}|\zeta_{t},\zeta_{t-1},...)
\\ &amp;= (\Pi')^{m}E(\zeta_t|\zeta_{t},\zeta_{t-1},...) + (\Pi')^{m-1}E(\zeta_{t+1} - E(\zeta_{t+1}|\zeta_t,\zeta_{t-1},...)|\zeta_t,\zeta_{t-1},...) +
\\ &amp; \hspace{0.5cm}...
\\ &amp; \hspace{0.5cm}+ \Pi'E(\zeta_{t+m-1} - E(\zeta_{t+m-1}|\zeta_{t+m-2},\zeta_{t-m-3},...)|\zeta_t,\zeta_{t-1},...)
\\ &amp; \hspace{0.5cm}+ E(\zeta_{t+m} - E(\zeta_{t+m}|\zeta_{t+m-1},\zeta_{t+m-2},...)|\zeta_t,\zeta_{t-1},...)
\\ &amp;= (\Pi')^{m}\zeta_t + (\Pi')^{m-1}[E(\zeta_{t+1}|\zeta_t,\zeta_{t-1},...) - E(E(\zeta_{t+1}|\zeta_t,\zeta_{t-1},...)|\zeta_t,\zeta_{t-1},...)] +
\\ &amp; \hspace{0.5cm}...
\\ &amp; \hspace{0.5cm}+ \Pi'[E(\zeta_{t+m-1}|\zeta_t,\zeta_{t-1},...) - E(E(\zeta_{t+m-1}|\zeta_{t+m-2},\zeta_{t-m-3},...)|\zeta_t,\zeta_{t-1},...)]
\\ &amp; \hspace{0.5cm}+ E(\zeta_{t+m}|\zeta_t,\zeta_{t-1},...) - E(E(\zeta_{t+m}|\zeta_{t+m-1},\zeta_{t+m-2},...)|\zeta_t,\zeta_{t-1},...)
\\ &amp;= (\Pi')^{m}\zeta_t.
\end{align*}\]</span> We could now also condition on other random variables, like <span class="math inline">\((Y_t, Y_{t-1},...)\)</span>. We summarize these random variables in a vector <span class="math inline">\(\mathcal{Y}_t\)</span>: <span class="math display">\[\begin{equation}
    \mathcal{Y}_t = (Y_t,Y_{t-1},...),
\end{equation}\]</span> the realisation of <span class="math inline">\(\mathcal{Y}_t\)</span> will be denoted as: <span class="math display">\[\begin{equation}
    \vec{y}_t = (y_t,y_{t-1},...).
\end{equation}\]</span> Furthmore we define: <span class="math display">\[\begin{equation}
    \mathcal{Y}_{t:\tau} = (Y_t,Y_{t-1},...,Y_{\tau}),
\end{equation}\]</span> the realisation of <span class="math inline">\(\mathcal{Y}_{t:\tau}\)</span> will be denoted as: <span class="math display">\[\begin{equation}
    \vec{y}_{t:\tau} = (y_t,y_{t-1},...,y_{\tau}).
\end{equation}\]</span> Generally speaking, <span class="math inline">\(\mathcal{Y}_T\)</span> will be the total time series of interest and <span class="math inline">\(\vec{y}_T\)</span> the observed realization. If the process is governed by regime <span class="math inline">\(S_t = j\)</span> at date <span class="math inline">\(t\)</span> then the conditional density of <span class="math inline">\(Y_t\)</span> is assumed to be given by <span class="math inline">\(f_{Y_t|S_t,\mathcal{Y}_{t-1};\alpha}(y_t|j,\vec{y}_{t-1})\)</span>. Thereby <span class="math inline">\(\alpha\)</span> is a vector of parameters characterizing the conditional density function. Furthermore it is assumed that the conditional density depends only on the current regime <span class="math inline">\(S_t\)</span> and not on past regimes, to be more precise it shall hold: <span class="math display">\[\begin{equation}
    f_{Y_t|S_t,\mathcal{Y}_{t-1};\alpha}(y_t|s_t,\vec{y}_{t-1}) = f_{Y_t|S_t,S_{t-1},...,\mathcal{Y}_{t-1};\alpha}(y_t|s_t,s_{t-1},...,\vec{y}_{t-1}).
\end{equation}\]</span> Additionally, <span class="math inline">\(S_{t+m}\)</span> shall be conditonally independent of <span class="math inline">\(\mathcal{Y}_t\)</span> given <span class="math inline">\(S_t\)</span>, for <span class="math inline">\(m \geq 1\)</span>, therefore it shall hold that: <span class="math display">\[\begin{equation}
    P_{\theta}(S_{t+m} = j|S_t = i) = P_{\theta}(S_{t+m} = j|S_t = i, \mathcal{Y}_t = \vec{y}_t).
\end{equation}\]</span> One could now condition on this random vector <span class="math inline">\(\mathcal{Y}_t\)</span>, given a parameter vector <span class="math inline">\(\theta\)</span>, which includes the transition probabilities of the Markov-Chain, as well as the parameters of the distribution of <span class="math inline">\(Y_t\)</span>, therefore <span class="math inline">\(\theta = (\Pi,\alpha)\)</span>. It should be noted that <span class="math inline">\(\Pi\)</span> has to be understood in this notation, as part of <span class="math inline">\(\theta\)</span>, as the vector of transition probabilities, instead of the matrix of transition probabilities. Still, we believe that this notation improves the readability and understanding of what <span class="math inline">\(\theta\)</span> is compared to other notations. We can now write: <span class="math display">\[\begin{equation}
\hat{\zeta}_{t|t} = E_{\theta}(\zeta_{t}|\mathcal{Y}_t = \vec{y}_t) =
\begin{pmatrix}
P_{\theta}(S_t = 1|\mathcal{Y}_t = \vec{y}_t)\\
P_{\theta}(S_t = 2|\mathcal{Y}_t = \vec{y}_t)\\
...\\
P_{\theta}(S_t = N|\mathcal{Y}_t = \vec{y}_t)
\end{pmatrix}.
\end{equation}\]</span> We now want to estimate <span class="math inline">\(\zeta_{t+m}\)</span> with <span class="math inline">\(\hat{\zeta}_{t+m|t} = E_{\theta}(\zeta_{t+m}|\mathcal{Y}_t = \vec{y}_t)\)</span>. From earlier we know that: <span class="math display">\[\begin{align*}
\displaystyle
(E_{\theta}(\zeta_{t+m}|\mathcal{Y}_t = \vec{y}_t))_j &amp;= P_{\theta}(S_{t+m} = j|\mathcal{Y}_t = \vec{y}_t)
\\&amp;= \sum_{i = 1}^{N}P_{\theta}(S_{t+m} = j,S_t = i|\mathcal{Y}_t = \vec{y}_t)
\\&amp;= \sum_{i = 1}^{N}P_{\theta}(S_{t+m} = j|S_t = i,\mathcal{Y}_t = \vec{y}_t) P_{\theta}(S_{t} = i|\mathcal{Y}_t = \vec{y}_t)
\\ &amp;= \sum_{i = 1}^{N}P_{\theta}(S_{t+m} = j|S_t = i)P_{\theta}(S_t = i|\mathcal{Y}_t = \vec{y}_t)
\\ &amp;= ((\Pi')^m)_{j,}\hat{\zeta}_{t|t}.
\end{align*}\]</span> Applying this to all elements we end up with: <span class="math display">\[\begin{equation}
    E_{\theta}(\zeta_{t+m}|\mathcal{Y}_t = \vec{y}_t) = (\Pi')^m\hat{\zeta}_{t|t},
\end{equation}\]</span> compare Hamilton (1994, page 693). This property will be essential later, therefore it is important to keep in mind that this indeed holds true.</p>
</section>
<section id="autoregressive-ar-processes" class="level2">
<h2 class="anchored" data-anchor-id="autoregressive-ar-processes">Autoregressive (AR) Processes</h2>
<p>Working with time series can be tricky, as one always deals with stochastic processes. The idea is the following, if one observes a time series <span class="math inline">\(\vec{y}_T = (y_1,...,y_T)\)</span>, then the observations are the realizations of the random variables <span class="math inline">\(\mathcal{Y}_T = (Y_1,...,Y_T)\)</span>. These random variables, are connected to each other, since they all stem from the same underlying stochastic process. The goal is now to estimate said stochastic process. Before estimating the parameters of a process, it is necessary to decide which process to assume as the underlying (or at least sufficiently similar) process. A rather often utilized process-family are AR processes, an AR(m) has the form: <span class="math display">\[\begin{align*}
    Y_t = c + \phi_1Y_{t-1} + ... + \phi_mY_{t-m} + U_t; \quad \text{where}\quad U_t \sim WN(0,\sigma^2).
\end{align*}\]</span> Thereby <span class="math inline">\(WN(0,\sigma^2)\)</span> denotes a zero-mean white noise process with variance <span class="math inline">\(\sigma^2\)</span>. The white noise distribution that is most often used is the normal distribution. Therefore <span class="math display">\[\begin{align*}
    Y_t = c + \phi_1Y_{t-1} + ... + \phi_mY_{t-m} + U_t; \quad \text{where}\quad U_t \sim N(0,\sigma^2),
\end{align*}\]</span> would qualify as an AR(m). This framework of an AR(m) with gaussian white noise will be the foundation on which Markov-Switching AR models are built in the next section.</p>
<p>#Markov-Switching Models (MSM)</p>
</section>
<section id="introduction-to-markov-switching-models" class="level2">
<h2 class="anchored" data-anchor-id="introduction-to-markov-switching-models">Introduction to Markov-Switching Models</h2>
<p>The basic idea of Markov-Switching models is that the stochastic process <span class="math inline">\(Y_1,..., Y_T\)</span> is itself influenced by another, underlying stochastic process, in this specific case by an underlying Markov-Chain. Therefore, a Markov-Switching AR(1) could take the following form: <span class="math display">\[\begin{equation}
Y_t = c_{s_t} + \phi_{s_t}Y_{t-1} + U_{t}; \quad \text{where} \quad U_{t} \overset{i.i.d.}{\sim} N(0,\sigma^2).
\end{equation}\]</span> We could alternatively write: <span class="math display">\[\begin{align*}
Y_t = X_t'\beta_{s_t} + U_t \quad \text{with} \quad
X_t =
\begin{pmatrix}
1
\\Y_{t-1}
\end{pmatrix}
\quad \text{and}
\quad \beta_{s_t} =
\begin{pmatrix}
c_{s_t}
\\\phi_{s_t}
\end{pmatrix}.
\end{align*}\]</span> Here <span class="math inline">\(S_t\)</span> follows a first-order Markov-Chain and <span class="math inline">\(s_t\)</span> is the value of the Markov-Chain at time <span class="math inline">\(t\)</span>. Furthermore, important assumptions are that (11) and (12) hold true, that there is a maximum lag order (in this case 1) as well as that <span class="math inline">\(U_t\)</span> shall be conditionally independent of <span class="math inline">\(S_{t-1},S_{t-2},...\)</span> given <span class="math inline">\(S_t\)</span> and that <span class="math inline">\(S_{t+m}\)</span> shall be conditionally independent of <span class="math inline">\(U_t,U_{t-1},...\)</span> given <span class="math inline">\(S_t\)</span>, for all <span class="math inline">\(m \geq 1\)</span>. To put it more formally, it shall hold that: <span class="math display">\[\begin{equation}
    f_{U_t|S_t;\alpha}(u_t|s_t) = f_{U_t|S_t,S_{t-1},...;\alpha}(u_t|s_t, s_{t-1},...),
\end{equation}\]</span> <span class="math display">\[\begin{equation}
    P_{\theta}(S_{t+m} = s_{t+m}|S_t =s_t) = P_{\theta}(S_{t+m} = s_{t+m}|S_t =s_t, U_t = u_t, U_{t-1} = u_{t-1},...).
\end{equation}\]</span> Additionally, a vector of the form of (7) would represent the vector of all observable variables until <span class="math inline">\(t\)</span>. It has to be emphasized that the density of <span class="math inline">\(Y_t\)</span> conditioned on <span class="math inline">\(S_t = s_t\)</span> and <span class="math inline">\(\mathcal{Y}_{t-1} = \vec{y}_{t-1}\)</span> has the following form: <span class="math display">\[\begin{equation}
\displaystyle
f_{Y_t|S_t,\mathcal{Y}_{t-1};\alpha}(y_t|s_t,\vec{y}_{t-1}) = \cfrac{1}{\sqrt{2\pi}\sigma}\exp\left(\cfrac{-(y_t - c_{s_t} - \phi_{s_t}y_{t-1})^2}{2\sigma^2}   \right).
\end{equation}\]</span> Here we can also see that (11) is indeed true for Markov-Switching AR(m) models with gaussian white noise, as long as earlier states of the Markov-Chain than the current state, <span class="math inline">\(s_t\)</span> don’t influence the parameters that describe the generation of <span class="math inline">\(Y_t\)</span>, i.e the intercept, the coefficients and the error-term variance. For this specific AR(1) <span class="math inline">\(\alpha\)</span> would consist of <span class="math inline">\(c_1,...,c_N,\phi_1,...,\phi_N\)</span> and <span class="math inline">\(\sigma^2\)</span>. We summarize the values of the conditional density functions for all potential states of the Markov-Chain in the following vector: <span class="math display">\[\begin{equation}
\eta_t = \begin{pmatrix}
f_{Y_t|S_t,\mathcal{Y}_{t-1}; \alpha}(y_t|1,\vec{y}_{t-1})\\
...\\
f_{Y_t|S_t, \mathcal{Y}_{t-1}; \alpha}(y_t|N,\vec{y}_{t-1})
\end{pmatrix}.
\end{equation}\]</span></p>
<p>Now that this model class has been introduced, we want to further investigate the question of optimal inference regarding the states of the Markov-Chain, often called regimes. In the following sections, the goal is to estimate the parameter vector <span class="math inline">\(\theta = (\Pi,\alpha)\)</span> given <span class="math inline">\(\mathcal{Y}_t = \vec{y}_{t}\)</span>.</p>
</section>
<section id="optimal-inference-of-the-regimes-and-derivation-of-the-log-likelihood" class="level2">
<h2 class="anchored" data-anchor-id="optimal-inference-of-the-regimes-and-derivation-of-the-log-likelihood">Optimal Inference of the Regimes and Derivation of the Log-Likelihood</h2>
<p>But before we follow this endeavour, we peak into a world where we assume that <span class="math inline">\(\theta\)</span> is known. Given <span class="math inline">\(\theta\)</span> we want to get inference regarding the regimes of the time series. We start by summarizing the <span class="math inline">\(P_{\theta}(S_t = j|\mathcal{Y}_t = \vec{y}_{t})\)</span> and <span class="math inline">\(P_{\theta}(S_{t+1} = j|\mathcal{Y}_t = \vec{y}_{t})\)</span> for all <span class="math inline">\(j = 1,...,N\)</span>, similarily to (13) in: <span class="math display">\[\begin{equation}
\hat{\zeta}_{t|t} = \begin{pmatrix}
P_{\theta}(S_t = 1|\mathcal{Y}_t = \vec{y}_{t})\\
...\\
P_{\theta}(S_t = N|\mathcal{Y}_t = \vec{y}_{t})\\
\end{pmatrix}
\quad \text{and} \quad
\hat{\zeta}_{t+1|t} = \begin{pmatrix}
P_{\theta}(S_{t+1} = 1|\mathcal{Y}_t = \vec{y}_{t})\\
...\\
P_{\theta}(S_{t+1} = N|\mathcal{Y}_t = \vec{y}_{t})\\
\end{pmatrix}.
\end{equation}\]</span> The claim Hamilton makes now is that the optimal inference can be derived by iterating over the following equations: <span class="math display">\[\begin{equation}
\displaystyle
\hat{\zeta}_{t|t} = \cfrac{(\hat{\zeta}_{t|t-1} \odot \eta_t)}{\mathbf{1'}(\hat{\zeta}_{t|t-1} \odot \eta_t)},
\end{equation}\]</span> <span class="math display">\[\begin{equation}
\displaystyle
\hat{\zeta}_{t+1|t} = \Pi'\hat{\zeta}_{t|t}.
\end{equation}\]</span> Where <span class="math inline">\(\odot\)</span> denotes element-by-element multiplication. In addition, the value of the Log-Likelihood function for the vector of all observations <span class="math inline">\(\vec{y}_{T}\)</span> at the point <span class="math inline">\(\theta\)</span> is gained as a side product: <span class="math display">\[\begin{equation}
\displaystyle
\mathcal{L}(\theta) = \sum_{t=1}^{T}\ln(f_{Y_t|\mathcal{Y}_{t-1};\theta}(y_t|\vec{y}_{t-1})),
\end{equation}\]</span> <span class="math display">\[\begin{equation}
\displaystyle
f_{Y_t|\mathcal{Y}_{t-1};\theta}(y_t|\vec{y}_{t-1}) = \mathbf{1'}(\hat{\zeta}_{t|t-1} \odot \eta_t),
\end{equation}\]</span> see Hamilton (1994, page 692). That this is indeed true is shown in the Appendix, section 9.1. Based on this system of two equations and a given <span class="math inline">\(\theta\)</span> we start with a random <span class="math inline">\(\hat{\zeta}_{1|0}\)</span> and iterate over all t until we reach T (T is the number of periods for which observations of the time series exist). This gives us the regime probabilities conditionally on the data until t. Additionally we get the Log-Likelihood function, which can be optimized in <span class="math inline">\(\theta\)</span> to derive the Maximum Likelihood estimate of <span class="math inline">\(\theta\)</span>. It is important to note that a direct optimization of <span class="math inline">\(\mathcal{L}(\theta)\)</span> can be computationally expensive and often leads to suboptimal results. Therefore, Hamilton introduced, in his paper “Analysis of Time Series subject to Changes in Regime” from 1990, an iterative optimization algorithm for <span class="math inline">\(\mathcal{L}(\theta)\)</span>, which is an application of the EM algorithm. Applying a specific variant of the EM algorithm to this optimization problem, instead of more general optimization algorithms can lead to better and computationally less expensive results, see Hamilton (1990, page 40-41). The details of this specific application of the EM algorithm for the optimization of <span class="math inline">\(\mathcal{L}(\theta)\)</span> are shown throughout the following sections, this includes a derivation of an algorithm developed by Kim (1994), which can improve some aspects of the application of the EM algorithm as shown in Hamilton (1990). The derivation shown of the “Kim-Algorithm” is based on Hamilton (1994, page 700-702).</p>
</section>
<section id="smoothed-inference-over-the-regimes" class="level2">
<h2 class="anchored" data-anchor-id="smoothed-inference-over-the-regimes">Smoothed Inference over the Regimes</h2>
<p>Before further investigating the optimization of the log-likelihood and the associated parameter estimation, we want to first discuss how to get inference on <span class="math inline">\(P_{\theta}(S_{t-1} = i|\mathcal{Y}_T = \vec{y}_{T})\)</span>, as this will be essential for the application of the aforementioned EM algorithm. Estimating <span class="math inline">\(P_{\theta}(S_{t-1} = i|\mathcal{Y}_T = \vec{y}_{T})\)</span> is often called “smoothed inference” for the regimes. To get this smoothed inference, we apply the algorithm from Kim (1994). The proposition is that, assuming <span class="math inline">\(S_t\)</span> follows a first order Markov-Chain and that the conditional density, <span class="math inline">\(f_{Y_t|S_t,\mathcal{Y}_{t-1};\alpha}(y_t|j,\vec{y}_{t-1})\)</span>, depends only on <span class="math inline">\(S_t,S_{t-1},...\)</span> through <span class="math inline">\(S_t\)</span>, i.e.&nbsp;that (11) holds true, an assumption we have already made throughout section 3.1, it shall hold that: <span class="math display">\[\begin{equation}
\displaystyle
\hat{\zeta}_{t|T} = \hat{\zeta}_{t|t} \odot (\Pi(\hat{\zeta}_{t+1|T}(\div)\hat{\zeta}_{t+1|t})),
\end{equation}\]</span> where <span class="math inline">\((\div)\)</span> is the symbol for element-by-element division, see Hamilton (1994, page 694). To get the values of <span class="math inline">\(P_{\theta}(S_{t-1} = i|\mathcal{Y}_T = \vec{y}_{T})\)</span> for <span class="math inline">\(t\)</span> in <span class="math inline">\(1,...,T\)</span> one starts with <span class="math inline">\(t = T-1\)</span> and iterates backwards. The derivation can be found in the Appendix, section 9.2.</p>
</section>
<section id="optimisation-of-the-conditional-log-likelihood" class="level2">
<h2 class="anchored" data-anchor-id="optimisation-of-the-conditional-log-likelihood">Optimisation of the Conditional Log-Likelihood</h2>
<section id="general-em-algorithm-theory" class="level3">
<h3 class="anchored" data-anchor-id="general-em-algorithm-theory">General EM Algorithm Theory</h3>
<p>We now turn to the EM algorithm. Assuming we observe <span class="math inline">\(\vec{y}_{T} = (y_1,...,y_T)\)</span>, a trick that is often utilized is to optimize a density function of the form <span class="math inline">\(f_{Y_T,...,Y_{m+1}|Y_{m},...,Y_1;\lambda}(y_T,...,y_{m+1}|y_m,...,y_1)\)</span> in <span class="math inline">\(\lambda\)</span> instead of <span class="math inline">\(f_{Y_T,...,Y_1;\theta}(y_T,...,y_1)\)</span> in <span class="math inline">\(\theta\)</span>. We have to optimize in <span class="math inline">\(\lambda\)</span> because if we choose such a conditional likelihood function, then we have to make assumptions about how the initial states <span class="math inline">\((Y_{m},...,Y_1)\)</span> are distributed. The simplest approach is to assume that they are seperately drawn from a distribution with the parameters <span class="math inline">\(\rho\)</span>. Thereby <span class="math inline">\(\rho\)</span> shall be unrelated of <span class="math inline">\(\Pi\)</span> and <span class="math inline">\(\alpha\)</span>. The new parameter vector <span class="math inline">\(\lambda\)</span> is therefore defined as <span class="math inline">\(\lambda = (\Pi,\alpha,\rho)\)</span>. The conditional likelihood function is primarily chosen due to practical reasons. Optimizing the likelihood function instead is often more challenging and yields next to no additional gain. Choosing the conditional likelihood enables the application of the EM algorithm, as described by Hamilton (1990), which estimates the parameters with relatively low computational demands, at least compared to other numerical methods, see Hamilton (1990, page 40). Still it has to be emphasized that the EM algorithm that Hamilton introduces only leads to a local maximum of the conditional likelihood function, as will be shown throughout this section. Generally speaking, this is not problematic, as one can start the algorithm with several different values to see whether the results are robust. We start by defining <span class="math display">\[\begin{equation}
P_{\lambda}(S_m = s_m,S_{m-1} = s_{m-1},...,S_1 = s_1|Y_m = y_m,...,Y_1 = y_1) = \rho_{s_m,...,s_1},
\end{equation}\]</span> and <span class="math display">\[\begin{equation}
\rho = (\rho_{1,1,..,1},\rho_{1,1,..,2},...,\rho_{N,N,..,N}).
\end{equation}\]</span> Thereby (26) is the vector of population probabilities, which are aggregated in (27). With this we can now derive the general EM algorithm: We assume we know nothing about <span class="math inline">\(\lambda\)</span> and it should be chosen such that the conditional likelihood <span class="math display">\[\begin{equation}
f_{\mathcal{Y}_{T:(m+1)}|\mathcal{Y}_{m};\lambda}(\vec{y}_{T:(m+1)}|\vec{y}_{m}) = f_{Y_T,...,Y_{m+1}|Y_m,...,Y_1;\lambda}(y_T,...,y_{m+1}|y_m,...,y_1),
\end{equation}\]</span> is maximized, the optimising <span class="math inline">\(\lambda\)</span> is called <span class="math inline">\(\lambda_{MLE}\)</span>. Furthermore we define <span class="math display">\[\begin{equation}
\displaystyle
\mathcal{S} = (S_T,S_{T-1},...,S_1),
\end{equation}\]</span> <span class="math display">\[\begin{equation}
\displaystyle
\vec{s}_T = (s_T,s_{T-1},...,s_1),
\end{equation}\]</span> <span class="math display">\[\begin{equation}
\displaystyle
Z_t = (S_t,S_{t-1},...,S_{t-m},Y_{t-1},...,Y_{t-m}),
\end{equation}\]</span> <span class="math display">\[\begin{equation}
\displaystyle
z_t = (s_t,s_{t-1},...,s_{t-m},y_{t-1},...,y_{t-m}),
\end{equation}\]</span> <span class="math display">\[\begin{equation}
\displaystyle
f_{\mathcal{Y}_{T:(m+1)},\mathcal{S}|\mathcal{Y}_m;\lambda}(\vec{y}_{T:(m+1)},\vec{s}_T|\vec{y}_m) = f_{Y_T,...,Y_{m+1},S_T,...,S_1|Y_m,...,Y_1;\lambda}(y_T,...,y_{m+1},s_T,...,s_1|y_m,...,y_1),
\end{equation}\]</span> <span class="math display">\[\begin{equation}
\displaystyle
\sum_{\vec{s}_T} P(\mathcal{S} = \vec{s}_T) \, = \sum_{s_T = 1}^{N}\cdots\sum_{s_1 = 1}^{N}P(S_T = s_T,...,S_1 = s_1),
\end{equation}\]</span> <span class="math display">\[\begin{equation}
\displaystyle
f_{\mathcal{Y}_{T:(m+1)}|\mathcal{Y}_{m};\lambda}(\vec{y}_{T:(m+1)}|\vec{y}_{m}) = \sum_{\vec{s}_T} f_{\mathcal{Y}_{T:(m+1)},\mathcal{S}|\mathcal{Y}_m;\lambda}(\vec{y}_{T:(m+1)},\vec{s}_T|\vec{y}_m),
\end{equation}\]</span> and <span class="math display">\[\begin{equation}
\displaystyle
Q_{\lambda_l,\vec{y}_T}(\lambda_{l+1}) = \sum_{\vec{s}_T} \ln(f_{\mathcal{Y}_{T:(m+1)},\mathcal{S}|\mathcal{Y}_m;\lambda_{l+1}}(\vec{y}_{T:(m+1)},\vec{s}_T|\vec{y}_m))f_{\mathcal{Y}_{T:(m+1)},\mathcal{S}|\mathcal{Y}_m;\lambda_{l}}(\vec{y}_{T:(m+1)},\vec{s}_T|\vec{y}_m).
\end{equation}\]</span> Where we call <span class="math inline">\(Q_{\lambda_l,\vec{y}_T}(\lambda_{l+1})\)</span> the expected log-likelihood. With that remark we finish the necessary notation, which is based on Hamilton (1990, page 42-44) and Hamilton (1990, page 46-47). Now we want to show that the EM algorithm works. It is noteworthy that the EM algorithm can be seen from two different perspectives.</p>
<p>First we want to discuss the EM algorithm as a sequence of optimization problems. <span class="math inline">\(\hat{\lambda}_l\)</span> is the result of the lth optimization problem, and we start with a random <span class="math inline">\(\hat{\lambda}_0\)</span> for the first optimization problem. We choose <span class="math inline">\(\hat{\lambda}_{l+1}\)</span> such that <span class="math inline">\(Q_{\hat{\lambda}_l,\vec{y}_T}(\lambda_{l+1})\)</span> is maximized. We remember: <span class="math display">\[\begin{align*}
\displaystyle
Q_{\hat{\lambda}_l,\vec{y}_T}(\lambda_{l+1}) = \sum_{\vec{s}_T} \ln(f_{\mathcal{Y}_{T:(m+1)},\mathcal{S}|\mathcal{Y}_m;\lambda_{l+1}}(\vec{y}_{T:(m+1)},\vec{s}_T|\vec{y}_m))f_{\mathcal{Y}_{T:(m+1)}|\mathcal{Y}_m;\hat{\lambda}_l}(\vec{y}_{T:(m+1)}|\vec{y}_m).
\end{align*}\]</span> Therefore <span class="math inline">\(\hat{\lambda}_{l+1}\)</span> fulfills: <span class="math display">\[\begin{equation}
\displaystyle
\sum_{\vec{s}_T} \cfrac{\partial \ln(f_{\mathcal{Y}_{T:(m+1)},\mathcal{S}|\mathcal{Y}_m;\lambda_{l+1}}(\vec{y}_{T:(m+1)},\vec{s}_T|\vec{y}_m))}{\partial\lambda_{l+1}}\Big|_{\lambda_{l+1} = \hat{\lambda}_{l+1}} \cdot f_{\mathcal{Y}_{T:(m+1)}|\mathcal{Y}_m;\hat{\lambda}_l}(\vec{y}_{T:(m+1)}|\vec{y}_m) = 0.
\end{equation}\]</span> It holds that <span class="math inline">\(\hat{\lambda}_{l+1}\)</span> is associated with an higher value of the conditional likelihood function than <span class="math inline">\(\hat{\lambda}_l\)</span> i.e.&nbsp;<span class="math inline">\(f_{\mathcal{Y}_{T:(m+1)}|\mathcal{Y}_m;\hat{\lambda}_{l+1}}(\vec{y}_{T:(m+1)}|\vec{y}_m) \geq f_{\mathcal{Y}_{T:(m+1)}|\mathcal{Y}_m;\hat{\lambda}_{l}}(\vec{y}_{T:(m+1)}|\vec{y}_m)\)</span>. In the following, we closely follow the derivation in Hamilton (1990, page 48-49). Per construction <span class="math inline">\(\hat{\lambda}_{l+1}\)</span> maximizes <span class="math inline">\(Q_{\hat{\lambda}_l,\vec{y}_T}(\lambda_{l+1})\)</span>, thus: <span class="math display">\[\begin{align*}
    Q_{\hat{\lambda}_l,\vec{y}_T}(\hat{\lambda}_{l+1}) \geq Q_{\hat{\lambda}_l,\vec{y}_T}(\hat{\lambda}_{l}) ; \quad \text{equal if} \quad \hat{\lambda}_{l+1} = \hat{\lambda}_l.
\end{align*}\]</span> We also note that <span class="math inline">\(\forall x \in \mathbb{R}^{+}: \ln(x) \leq (x-1)\)</span>. This is because we can show that <span class="math inline">\(h(x) = x - 1 - \ln(x)\)</span> has a minimum at <span class="math inline">\(x = 1\)</span> and <span class="math inline">\(h(1)\)</span> = 0. The first order condition is given by: <span class="math display">\[\begin{align*}
h'(x) &amp;= 1 - \cfrac{1}{x} = 0
\\&amp; \Leftrightarrow x = 1.
\end{align*}\]</span> That this is indeed a minimum can be shown by checking the second derivative at the point <span class="math inline">\(x = 1\)</span>: <span class="math display">\[\begin{align*}
h''(1) = \cfrac{1}{1^2} &gt; 0,
\end{align*}\]</span> thus <span class="math inline">\(h(x)\)</span> has a minimum at the point <span class="math inline">\(x = 1\)</span> and <span class="math inline">\(\forall x \in \mathbb{R}^{+}: \ln(x) \leq (x-1)\)</span> is therefore a true statement. We can apply this now and show: <span class="math display">\[\begin{align*}
\displaystyle
0 &amp;\leq Q_{\hat{\lambda}_l,\vec{y}_T}(\hat{\lambda}_{l+1}) - Q_{\hat{\lambda}_l, \vec{y}_T}(\hat{\lambda}_l)
\\&amp;= \sum_{\vec{s}_T}\ln(f_{\mathcal{Y}_{T:(m+1)},\mathcal{S}|\mathcal{Y}_m;\hat{\lambda}_{l+1}}(\vec{y}_{T:(m+1)},\vec{s}_T|\vec{y}_m))f_{\mathcal{Y}_{T:(m+1)},\mathcal{S}|\mathcal{Y}_m;\hat{\lambda}_l}(\vec{y}_{T:(m+1)},\vec{s}_T|\vec{y}_m)
\\&amp; \hspace{0.5cm} - \sum_{\vec{s}_T}\ln(f_{\mathcal{Y}_{T:(m+1)},\mathcal{S}|\mathcal{Y}_m;\hat{\lambda}_l}(\vec{y}_{T:(m+1)},\vec{s}_T|\vec{y}_m))f_{\mathcal{Y}_{T:(m+1)},\mathcal{S}|\mathcal{Y}_m;\hat{\lambda}_l}(\vec{y}_{T:(m+1)},\vec{s}_T|\vec{y}_m)
\\ &amp;= \sum_{\vec{s}_T} \ln\left[\cfrac{f_{\mathcal{Y}_{T:(m+1)},\mathcal{S}|\mathcal{Y}_m;\hat{\lambda}_{l+1}}(\vec{y}_{T:(m+1)},\vec{s}_T|\vec{y}_m)}{f_{\mathcal{Y}_{T:(m+1)},\mathcal{S}|\mathcal{Y}_m;\hat{\lambda}_{l}}(\vec{y}_{T:(m+1)},\vec{s}_T|\vec{y}_m)}\right]f_{\mathcal{Y}_{T:(m+1)},\mathcal{S}|\mathcal{Y}_m;\hat{\lambda}_{l}}(\vec{y}_{T:(m+1)},\vec{s}_T|\vec{y}_m)
\\ &amp;\leq \sum_{\vec{s}_T} \left[\cfrac{f_{\mathcal{Y}_{T:(m+1)},\mathcal{S}|\mathcal{Y}_m;\hat{\lambda}_{l+1}}(\vec{y}_{T:(m+1)},\vec{s}_T|\vec{y}_m)}{f_{\mathcal{Y}_{T:(m+1)},\mathcal{S}|\mathcal{Y}_m;\hat{\lambda}_{l}}(\vec{y}_{T:(m+1)},\vec{s}_T|\vec{y}_m)} - 1\right]f_{\mathcal{Y}_{T:(m+1)},\mathcal{S}|\mathcal{Y}_m;\hat{\lambda}_{l}}(\vec{y}_{T:(m+1)},\vec{s}_T|\vec{y}_m)
\\ &amp;= \sum_{\vec{s}_T} (f_{\mathcal{Y}_{T:(m+1)},\mathcal{S}|\mathcal{Y}_m;\hat{\lambda}_{l+1}}(\vec{y}_{T:(m+1)},\vec{s}_T|\vec{y}_m)-f_{\mathcal{Y}_{T:(m+1)},\mathcal{S}|\mathcal{Y}_m;\hat{\lambda}_l}(\vec{y}_{T:(m+1)},\vec{s}_T|\vec{y}_m))
\\ &amp;= f_{\mathcal{Y}_{T:(m+1)}|\mathcal{Y}_m;\hat{\lambda}_{l+1}}(\vec{y}_{T:(m+1)}|\vec{y}_m)-f_{\mathcal{Y}_{T:(m+1)}|\mathcal{Y}_m;\hat{\lambda}_{l}}(\vec{y}_{T:(m+1)}|\vec{y}_m).
\end{align*}\]</span> With that we have shown that the algorithm indeed leads to an increase of the conditional likelihood function with each step. Now we want to show that if <span class="math inline">\(\hat{\lambda}_{l+1} = \hat{\lambda}_{l}\)</span>, then the first order condition for maximizing <span class="math inline">\(f_{\mathcal{Y}_{T:(m+1)}|\mathcal{Y}_{m};\lambda}(\vec{y}_{T:(m+1)}|\vec{y}_{m})\)</span> is fulfilled by <span class="math inline">\(\lambda = \hat{\lambda}_l\)</span>. This is the case because if: <span class="math display">\[\begin{align*}
\displaystyle
\cfrac{\partial Q_{\hat{\lambda}_l,\vec{y}_T}(\lambda_{l+1})}{\partial \lambda_{l+1}}\Big|_{\lambda_{l+1} = \hat{\lambda}_l} = 0.
\end{align*}\]</span> Then: <span class="math display">\[\begin{align*}
\displaystyle
\cfrac{\partial f_{\mathcal{Y}_{T:(m+1)}|\mathcal{Y}_{m};\lambda}(\vec{y}_{T:(m+1)}|\vec{y}_{m})}{\partial \lambda}\Big|_{\lambda = \hat{\lambda}_l} = 0,
\end{align*}\]</span> this holds true because: <span class="math display">\[\begin{align*}
\displaystyle
\cfrac{\partial Q_{\hat{\lambda}_l,\vec{y}_T}(\lambda_{l+1})}{\partial \lambda_{l+1}}\Big|_{\lambda_{l+1} = \hat{\lambda}_l}
&amp;= \sum_{\vec{s}_T} \left(\cfrac{1}{f_{\mathcal{Y}_{T:(m+1)},\mathcal{S}|\mathcal{Y}_m;\lambda_{l+1}}(\vec{y}_{T:(m+1)},\vec{s}_T|\vec{y}_m)} \cfrac{\partial f_{\mathcal{Y}_{T:(m+1)},\mathcal{S}|\mathcal{Y}_m;\lambda_{l+1}}(\vec{y}_{T:(m+1)},\vec{s}_T|\vec{y}_m)}{\partial \lambda_{l+1}}\right)\Big|_{\lambda_{l+1} = \hat{\lambda}_l}
\\&amp; \hspace{0.5cm} \cdot f_{\mathcal{Y}_{T:(m+1)},\mathcal{S}|\mathcal{Y}_m;\hat{\lambda}_l}(\vec{y}_{T:(m+1)},\vec{s}_T|\vec{y}_m)
\\ &amp;= \sum_{\vec{s}_T} \cfrac{\partial f_{\mathcal{Y}_{T:(m+1)},\mathcal{S}|\mathcal{Y}_m;\lambda_{l+1}}(\vec{y}_{T:(m+1)},\vec{s}_T|\vec{y}_m)}{\partial \lambda_{l+1}}\Big|_{\lambda_{l+1} = \hat{\lambda}_l}
\\ &amp;= \cfrac{\partial f_{\mathcal{Y}_{T:(m+1)}|\mathcal{Y}_m;\lambda_{l+1}}(\vec{y}_{T:(m+1)}|\vec{y}_m)}{\partial \lambda_{l+1}}\Big|_{\lambda_{l+1} = \hat{\lambda}_l}.
\end{align*}\]</span> With that in mind, we proceed to consider the second perspective. One could say that the EM algorithm replaces the unobserved scores with their conditional expectation. Assuming we know <span class="math inline">\(\vec{s}_T\)</span>, then <span class="math inline">\(\hat{\lambda}_{MLE}(\vec{s}_T)\)</span> is characterized by the first-order condition: <span class="math display">\[\begin{align*}
\displaystyle
\cfrac{\partial \ln(f_{\mathcal{Y}_{T:(m+1)},\mathcal{S}|\mathcal{Y}_m;\lambda}(\vec{y}_{T:(m+1)},\vec{s}_T|\vec{y}_m))}{\partial \lambda}\Big|_{\lambda = \hat{\lambda}_{MLE}(\vec{s}_T)} = 0.
\end{align*}\]</span> Even so we have no data regarding <span class="math inline">\(\mathcal{S}\)</span> we still have inference regarding <span class="math inline">\(\mathcal{S}\)</span>, at least for the lth step, based on <span class="math inline">\(\hat{\lambda}_l\)</span> and <span class="math inline">\(\mathcal{Y}_T = \vec{y}_T\)</span>. We can formulate: <span class="math display">\[\begin{equation*}
\displaystyle
P_{\hat{\lambda}_l}(\mathcal{S} = \vec{s}_T|\mathcal{Y}_T = \vec{y}_T) = \cfrac{f_{\mathcal{Y}_{T:(m+1)},\mathcal{S}|\mathcal{Y}_{m};\hat{\lambda}_l}(\vec{y}_{T:(m+1)},\vec{s}_T|\vec{y}_m)}{f_{\mathcal{Y}_{T:(m+1)}|\mathcal{Y}_m;\hat{\lambda}_{l}}(\vec{y}_{T:(m+1)}|\vec{y}_m)}.
\end{equation*}\]</span> For all possible values of <span class="math inline">\(\mathcal{S}\)</span>, which amounts to <span class="math inline">\(N^T\)</span> possibilities, there exists such a first-order condition. If one weights all these FOCs with the probability <span class="math inline">\(P_{\hat{\lambda}_l}(\mathcal{S} = \vec{s}_T|\mathcal{Y}_T = \vec{y}_T)\)</span> then we choose <span class="math inline">\(\lambda\)</span> such that: <span class="math display">\[\begin{align*}
\displaystyle
&amp; \sum_{\vec{s}_T} \cfrac{\partial \ln(f_{\mathcal{Y}_{T:(m+1)},\mathcal{S}|\mathcal{Y}_m;\lambda}(\vec{y}_{T:(m+1)},\vec{s}_T|\vec{y}_m))}{\partial \lambda} \cdot \cfrac{f_{\mathcal{Y}_{T:(m+1)},\mathcal{S}|\mathcal{Y}_{m};\hat{\lambda}_l}(\vec{y}_{T:(m+1)},\vec{s}_T|\vec{y}_m)}{f_{\mathcal{Y}_{T:(m+1)}|\mathcal{Y}_m;\hat{\lambda}_{l}}(\vec{y}_{T:(m+1)}|\vec{y}_m)} = 0
\\ &amp;\Leftrightarrow \cfrac{1}{f_{\mathcal{Y}_{T:(m+1)}|\mathcal{Y}_m;\hat{\lambda}_{l}}(\vec{y}_{T:(m+1)}|\vec{y}_m)}\cfrac{\partial Q_{\hat{\lambda}_l,\vec{y}_T}(\lambda)}{\partial \lambda} = 0
\\ &amp;\Leftrightarrow \cfrac{\partial Q(\lambda,\hat{\lambda}_l,\vec{y}_T)}{\partial \lambda} = 0.
\end{align*}\]</span> This again is the characterizing condition for <span class="math inline">\(\hat{\lambda}_{l+1}\)</span> in the first perspective!</p>
</section>
<section id="application-of-the-em-algorithm-to-markov-switching-ar-models" class="level3">
<h3 class="anchored" data-anchor-id="application-of-the-em-algorithm-to-markov-switching-ar-models">Application of the EM Algorithm to Markov-Switching AR Models</h3>
<p>The next essential step in the theoretical exposition is to demonstrate the application of the EM algorithm to models of the type we are using, that is, models with an underlying Markov-Chain and a dependence on a maximum lag order. Hamilton states that, when using the EM algorithm to maximize the conditional log-likelihood, one obtains three equations to iterate over, see Hamilton (1990, page 51). The equations given by Hamilton are: <span class="math display">\[\begin{equation}
\displaystyle
\pi_{i,j}^{(l+1)} = \cfrac{\sum_{t = m+1}^{T}P_{\lambda_l}(S_t = j,S_{t-1} = i|\mathcal{Y}_T = \vec{y}_T)}{\sum_{t = m+1}^{T}P_{\lambda_l}(S_{t-1} = i|\mathcal{Y}_T = \vec{y}_T)},  
\end{equation}\]</span> <span class="math display">\[\begin{equation}
\displaystyle
\sum_{t = m+1}^{T}\sum_{s_t = 1}^{N}\cdots\sum_{s_{t-m} = 1}^{N}\cfrac{\partial \ln(f_{Y_t|Z_t;\alpha}(y_t|z_t))}{\partial \alpha}\Big|_{\alpha = \alpha_{l+1}} \cdot P_{\lambda_l}(S_t = s_t,...,S_{t-m} = s_{t-m}|\mathcal{Y}_T = \vec{y}_T) = 0,
\end{equation}\]</span> <span class="math display">\[\begin{equation}
\displaystyle
\rho_{i_m,...,i_1}^{(l+1)} = P_{\lambda_l}(S_m = i_m,...,S_1 = i_1|\mathcal{Y}_T = \vec{y}_T).  
\end{equation}\]</span></p>
<p>We show that these equations are indeed true in the Appendix, section 9.3. Given this exposition one can apply the EM algorithm to a very broad class of models, even broader than just Markov-Switching AR models, but to actually estimate a specific model, one has to specify the assumed process in more detail. Hamilton (1990) shows only one potential setup for Markov-Switching AR models, we will call this setup “Example 0” throughout this paper. Deriving the results for Example 0 will be the subject of the next section. After this is done we will show 5 more examples, establishing broader AR setups, until the theory for estimating any potential Markov-Switching AR model has been shown. Thereby, Example 1 to Example 3 are specific examples that are introduced to improve the readability of the general cases, Example 4 and Example 5.</p>
</section>
<section id="example-0-switching-coefficients-and-intercept-non-switching-sigma2" class="level3">
<h3 class="anchored" data-anchor-id="example-0-switching-coefficients-and-intercept-non-switching-sigma2">Example 0: Switching Coefficients and Intercept, Non-Switching <span class="math inline">\(\sigma^2\)</span></h3>
<p>Here we assume that the underlying process is given by a Markov-Switching AR(m), which fulfills all assumptions formulated in section 4.1, the only difference is that the assumed underlying AR process now has the following form. <span class="math display">\[\begin{equation}
\displaystyle
Y_t = c_{s_t}+\phi_{1,s_t}Y_{t-1} +...+ \phi_{m,s_t}Y_{t-m} + U_t \quad \text{where} \quad  U_{t} \overset{i.i.d.}{\sim} N(0,\sigma^2).
\end{equation}\]</span> We could alternatively write: <span class="math display">\[\begin{align*}
Y_t = X_t'\beta_{s_t} + U_t \quad \text{with} \quad
X_t =
\begin{pmatrix}
1
\\Y_{t-1}
\\...
\\Y_{t-m}
\end{pmatrix}
\quad \text{and}
\quad \beta_{s_t} =
\begin{pmatrix}
c_{s_t}
\\\phi_{1,s_t}
\\...
\\\phi_{m,s_t}
\end{pmatrix}.
\end{align*}\]</span> The following derivation closely follows Hamilton (1990, page 56-58). First, we see that: <span class="math display">\[\begin{align*}
\displaystyle
f_{Y_t|Z_t;\alpha}(y_t|z_t) = \cfrac{1}{\sqrt{2\pi}\sigma} \exp\left(\cfrac{-(y_t-x_t'\beta_{s_t})^2}{2\sigma^2}\right).
\end{align*}\]</span> It is important to note that <span class="math inline">\(x_t\)</span> denotes the vector of realizations for <span class="math inline">\(X_t\)</span>. Then it holds that: <span class="math display">\[\begin{equation}
\displaystyle
\cfrac{\partial \ln(f_{Y_t|Z_t;\alpha}(y_t|z_t))}{\partial \beta_j} =
\begin{cases}
    \cfrac{(y_t - x_t'\beta_j)x_t}{\sigma^2}, &amp; \text{if $S_t = j$}\\
    0, &amp; \text{otherwise}
\end{cases},
\end{equation}\]</span> <span class="math display">\[\begin{equation}
\cfrac{\partial \ln(f_{Y_t|Z_t;\alpha}(y_t|z_t))}{\partial \sigma^{-2}} = \cfrac{\sigma^2}{2} - \cfrac{(y_t -x_t'\beta_{s_t})^2}{2}.
\end{equation}\]</span> (43) is indeed true, because: <span class="math display">\[\begin{align*}
\displaystyle
\ln\left[\frac{1}{\sqrt{2\pi}\sigma} \exp\left(\frac{-(y_t-x_t'\beta_{s_t})^2}{2\sigma^2}\right)\right] &amp;= \ln\left(\frac{1}{\sqrt{2\pi}\sigma}\right)-\frac{1}{2}\left(\frac{(y_t-x_t'\beta_{s_t})^2}{\sigma^2}\right)
\\ &amp;= \ln(1) - \ln(\sqrt{2\pi}\sigma) - \cfrac{1}{2}(y_t - x_t'\beta_{s_t})^2\cfrac{1}{\sigma^2}
\\ &amp;= -\ln(\sqrt{2\pi}) -\ln(\sigma) - \cfrac{1}{2}(y_t-x_t'\beta_{s_t})^2\cfrac{1}{\sigma^2}
\\ &amp;= -\ln(\sqrt{2\pi}) - \ln\left(\left(\frac{1}{\sigma^2}\right)^{-\frac{1}{2}}\right) - \cfrac{1}{2}(y_t-x_t'\beta_{s_t})^2\cfrac{1}{\sigma^2}.
\end{align*}\]</span> And thus: <span class="math display">\[\begin{align*}
\cfrac{\partial \ln\left[\frac{1}{\sqrt{2\pi}\sigma} \exp\left(\frac{-(y_t-x_t'\beta_{s_t})^2}{2\sigma^2}\right)\right]}{\partial \left(\frac{1}{\sigma^2}\right)} &amp;= \left(\frac{1}{\sigma^2}\right)^{\frac{1}{2}}\left(\frac{1}{2}\left(\frac{1}{\sigma^2}\right)^{-\frac{3}{2}}\right)-\frac{1}{2}(y_t-x_t'\beta_{s_t})^2
\\ &amp;= \cfrac{\sigma^2}{2} - \cfrac{(y_t-x_t'\beta_{s_t})^2}{2}.
\end{align*}\]</span> We insert these results into (39), which leads to: <span class="math display">\[\begin{equation}
    \displaystyle
    \sum_{t = m+1}^{T}\cfrac{(y_t-x_t'\beta_j^{(l+1)})x_t}{\sigma_{(l+1)}^2} P_{\lambda_l}(S_t = j|\mathcal{Y}_T = \vec{y}_T) = 0,
    \end{equation}\]</span> and <span class="math display">\[\begin{equation}
    \displaystyle
\sigma_{(l+1)}^2 = \sum_{t = m+1}^{T}\sum_{s_t = 1}^{N}\cfrac{(y_t\sqrt{P_{\lambda_l}(S_t = s_t|\mathcal{Y}_T = \vec{y}_T)} - x_t'\sqrt{P_{\lambda_l}(S_t = s_t|\mathcal{Y}_T = \vec{y}_T)}\beta_{s_t}^{(l+1)})^2}{(T-m)}.
    \end{equation}\]</span></p>
<p>(44) is true because (42) can be understood as a function of <span class="math inline">\(S_t\)</span>, we could write <span class="math display">\[\begin{align*}
\cfrac{\partial \ln(f_{Y_t|Z_t;\alpha}(y_t|z_t))}{\partial \beta_j} = g_{j}(S_t) =
\begin{cases}
    \cfrac{(y_t - x_t'\beta_j)x_t}{\sigma^2}, &amp; \text{if $S_t = j$}\\
    0, &amp; \text{otherwise}
\end{cases},
\end{align*}\]</span> thus we can write: <span class="math display">\[\begin{align*}
&amp; \sum_{t = m+1}^{T}\sum_{s_t = 1}^{N}\sum_{s_{t-1} = 1}^{N}\cdots\sum_{s_{t-m} = 1}^{N}\cfrac{\partial \ln(f_{Y_t|Z_t;\alpha}(y_t|z_t))}{\partial \beta_{j}}\Big|_{\alpha = \alpha^{(l+1)}}    P_{\lambda_l}(S_t = s_t,...,S_{t-m} = s_{t-m}|\mathcal{Y}_T = \vec{y}_T) = 0
\\ &amp; \Leftrightarrow \sum_{t = m+1}^{T}\sum_{s_t = 1}^{N}\sum_{s_{t-1} = 1}^{N}\cdots\sum_{s_{t-m} = 1}^{N} g_j(s_t)     P_{\lambda_l}(S_t = s_t,...,S_{t-m} = s_{t-m}|\mathcal{Y}_T = \vec{y}_T) = 0
\\ &amp; \Leftrightarrow   \sum_{t = m+1}^{T}\sum_{s_t = 1}^{N}g_j(s_t)\sum_{s_{t-1} = 1}^{N}\cdots\sum_{s_{t-m} = 1}^{N} P_{\lambda_l}(S_t = s_t,...,S_{t-m} = s_{t-m}|\mathcal{Y}_T = \vec{y}_T) = 0
\\ &amp; \Leftrightarrow   \sum_{t = m+1}^{T}\sum_{s_t = 1}^{N}g_j(s_t) P_{\lambda_l}(S_t = s_t|\mathcal{Y}_T = \vec{y}_T) = 0
\\ &amp; \Leftrightarrow   \sum_{t = m+1}^{T}\cfrac{(y_t-x_t'\beta_{j}^{(l+1)})x_t}{\sigma_{(l+1)}^2} P_{\lambda_l}(S_t = j|\mathcal{Y}_T = \vec{y}_T) = 0.
\end{align*}\]</span></p>
<p>And (45) is true because: <span class="math display">\[\begin{align*}
    &amp;\sum_{t = m+1}^{T}\sum_{s_t = 1}^{N}\cdots\sum_{s_{t-m} = 1}^{N}\left(\cfrac{\sigma_{(l+1)}^2}{2}-\cfrac{(y_t-x_t'\beta_{s_t}^{(l+1)})^2}{2}\right)P_{\lambda_l}(S_t = s_t,...,S_{t-m} = s_{t-m}|\mathcal{Y}_T = \vec{y}_T) = 0
    \\ &amp;\Leftrightarrow \sum_{t = m+1}^{T}\sum_{s_t = 1}^{N}\left(\cfrac{\sigma_{(l+1)}^2}{2} - \cfrac{(y_t -x_t'\beta_{s_t}^{(l+1)})^2}{2}\right)P_{\lambda_l}(S_t = s_t|\mathcal{Y}_T = \vec{y}_T) = 0
    \\ &amp;\Leftrightarrow \sum_{t = m+1}^{T}\sum_{s_t = 1}^{N}P_{\lambda_l}(S_t = s_t|\mathcal{Y}_T = \vec{y}_T)\cfrac{\sigma_{(l+1)}^2}{2} = \sum_{t = m+1}^{T}\sum_{s_t = 1}^{N} \cfrac{(y_t -x_t'\beta_{s_t}^{(l+1)})^2}{2}P_{\lambda_l}(S_t = s_t|\mathcal{Y}_T = \vec{y}_T)
    \\&amp; \Leftrightarrow (T-m)\cfrac{\sigma_{(l+1)}^2}{2} = \sum_{t = m+1}^{T}\sum_{s_t = 1}^{N}\cfrac{(y_t - x_t'\beta_{s_t}^{(l+1)})^2}{2}P_{\lambda_l}(S_t = s_t|\mathcal{Y}_T = \vec{y}_T)
\\ &amp;\Leftrightarrow \sigma_{(l+1)}^2 = \sum_{t = m+1}^{T}\sum_{s_t = 1}^{N}\cfrac{(y_t - x_t'\beta_{s_t}^{(l+1)})^2}{(T-m)}P_{\lambda_l}(S_t = s_t|\mathcal{Y}_T = \vec{y}_T)
\\ &amp;\Leftrightarrow \sigma_{(l+1)}^2 = \sum_{t = m+1}^{T}\sum_{s_t = 1}^{N}\cfrac{(y_t\sqrt{P_{\lambda_l}(S_t = s_t|\mathcal{Y}_T = \vec{y}_T)} - x_t'\sqrt{P_{\lambda_l}(S_t = s_t|\mathcal{Y}_T = \vec{y}_T)}\beta_{s_t}^{(l+1)})^2}{(T-m)}.
\end{align*}\]</span> Conveniently we can estimate in this specific case all parameters via an OLS Regression. Let us assume we have the parameter vector from the previous iteration <span class="math inline">\(\lambda_l\)</span> (to start the algorithm one starts with a random <span class="math inline">\(\lambda_0\)</span>), we first define: <span class="math display">\[\begin{align*}
\displaystyle
y_t^* = y_t\sqrt{P_{\lambda_l}(S_t = j|\mathcal{Y}_T = \vec{y}_T)} \quad \text{and} \quad x_t^* = x_t\sqrt{P_{\lambda_l}(S_t = j|\mathcal{Y}_T = \vec{y}_T)}.
\end{align*}\]</span> Then we regress <span class="math inline">\(y_t^*\)</span> on <span class="math inline">\(x_t^*\)</span>. Thus <span class="math inline">\(\sum_{t = m+1}^{T}(y_t^* - x_t'^*\beta_j)^2\)</span> should be minimized, which leads to the following FOC that characterises <span class="math inline">\(\beta_j^{(l+1)}\)</span>: <span class="math display">\[\begin{align*}
&amp;\sum_{t = m+1}^{T}2(y_t^* - (x_t^*)'\beta_j^{(l+1)})(-1)x_t^* = 0
\\ &amp;\Leftrightarrow \sum_{t = m+1}^{T}(y_t\sqrt{P_{\lambda_l}(S_t = j |\mathcal{Y}_T = \vec{y}_T)} - x_t'\sqrt{P_{\lambda_l}(S_t = j |\mathcal{Y}_T = \vec{y}_T)}\beta_j^{(l+1)})x_t\sqrt{P_{\lambda_l}(S_t = j |\mathcal{Y}_T = \vec{y}_T)} = 0
\\ &amp;\Leftrightarrow \sum_{t = m+1}^{T}(y_t - x_t'\beta_j^{(l+1)}) x_t P_{\lambda_l}(S_t = j|\mathcal{Y}_T = \vec{y}_T) = 0.
\end{align*}\]</span> Which is equivalent to conditon (44). This is now done <span class="math inline">\(N\)</span> times to estimate <span class="math inline">\(\beta_1^{(l+1)},...,\beta_N^{(l+1)}\)</span>. By squaring and summing the residuals we get our estimate of <span class="math inline">\(\sigma^2\)</span>: <span class="math display">\[\begin{align*}
\displaystyle
\sigma_{(l+1)}^2 = \cfrac{1}{(T-m)}\sum_{j = 1}^{N}\sum_{t = m+1}^{T}(y_t^* - (x_t^*)'\beta_j^{(l+1)})^2.
\end{align*}\]</span></p>
<p>Summarizing one can say, that we achieve our estimate for the <span class="math inline">\(\beta_j\)</span> of the next iteration by solving the following optimization problem: <span class="math display">\[\begin{equation}
\displaystyle
    \arg\min_{\beta_j} \sum_{t = m+1}^{T}(y_t^* - x_t'^*\beta_j)^2.
\end{equation}\]</span> And then calculate our estimate of the <span class="math inline">\(\sigma^2\)</span> of the next iteration with: <span class="math display">\[\begin{equation}
\displaystyle
    \sigma^2 = \cfrac{1}{(T-m)}\sum_{j = 1}^{N}\sum_{t = m+1}^{T}(y_t^* - (x_t^*)'\beta_j)^2.
\end{equation}\]</span> Therefore, we have now a specific algorithm for estimating the parameters of a Markov-Switching AR model, where all coefficients switch and the error term variance does not switch. This is the case presented in Hamilton (1990, page 56-58). Sadly Hamilton does not show how to apply the EM algorithm to broader structures of AR models, therefore the following examples are the application of the EM algorithm to broader defined AR processes. To the best of the authors knowledge this presentation of applying the EM algorithm to broader defined specific AR processes is novel.</p>
</section>
<section id="example-1-non-switching-intercept" class="level3">
<h3 class="anchored" data-anchor-id="example-1-non-switching-intercept">Example 1: Non-Switching Intercept</h3>
<p>From the previous derivations, we know that the equations (38), (39), and (40) hold. The five applications of the EM algorithm that we now specifically present are within the class of models for which these three equations were originally derived, i.e.&nbsp;an autoregressive process with a maximum lag order. As opposed to Example 0 we vary the parameters influenced by the underlying Markov-Chain. It is important to emphasize that all models presented are still Markov-Switching AR(m) models with gaussian white noise that fulfill the assumptions made in section 4.1, i.e that fulfill (11), (12), (16) and (17). Assumption (11) is fulfilled because the parameters that describe the generation of <span class="math inline">\(Y_t\)</span> are only directly influenced by the current state of the Markov-Chain <span class="math inline">\(s_t\)</span> and not by earlier states of the Markov-Chain <span class="math inline">\(s_{t-1},s_{t-2},...\)</span>, as described in 4.1.\\ With that general short discussion out of the way we now turn to Example 1. This time we assume that the coefficients switch, while the intercept and the error term variance do not switch. The assumed process therefore has the following form: <span class="math display">\[\begin{align*}
    Y_t = c + \phi_{1,s_t}Y_{t-1}+...+\phi_{m,s_t}Y_{t-m} + U_t; \quad \text{where} \quad U_{t} \overset{i.i.d.}{\sim} N(0,\sigma^2).
\end{align*}\]</span> We could alternatively write: <span class="math display">\[\begin{align*}
Y_t = c + X_t'\phi_{s_t} + U_t \quad \text{with} \quad
X_t =
\begin{pmatrix}
    Y_{t-1}
    \\Y_{t-2}
    \\...
    \\Y_{t-m}
\end{pmatrix}, \quad
\phi_{s_t} =
\begin{pmatrix}
    \phi_{1,s_t}
    \\\phi_{2,s_t}
    \\...
    \\\phi_{m,s_t}
\end{pmatrix} \quad \text{and} \quad
\beta_{s_t} =
\begin{pmatrix}
    c
    \\\phi_{1,s_t}
    \\\phi_{2,s_t}
    \\...
    \\\phi_{m,s_t}
\end{pmatrix}.
\end{align*}\]</span></p>
<p>We start again with the conditional log-likelihood, which has the following form: <span class="math display">\[\begin{align*}
\displaystyle
\ln(f_{Y_t|Z_t;\alpha}(y_t|z_t)) = \ln(\cfrac{1}{\sqrt{2\pi}\sigma}) - \cfrac{(y_t - c - x_t'\phi_{s_t})^2}{2\sigma^2}.
\end{align*}\]</span> Now we approach this very similar to how we approached Example 0, we take the derivative in <span class="math inline">\(\alpha\)</span>, only that now there is a switching and a non-switching part in <span class="math inline">\(\beta\)</span>, we have to take the derivative in each. It holds that: <span class="math display">\[\begin{align*}
\displaystyle
\cfrac{\partial \ln(f_{Y_t|Z_t;\alpha}(y_t|z_t))}{\partial c} = \cfrac{(y_t - c -x_t'\phi_{s_t})}{\sigma^2} \quad \forall s_t \in \{1,...,N\}.
\end{align*}\]</span> We substitute our result in (39): <span class="math display">\[\begin{align*}
\displaystyle
&amp;\sum_{t = m+1}^{T}\sum_{s_t = 1}^{N}\cdots\sum_{s_{t-m} = 1}^{N}\cfrac{(y_t - c_{(l+1)} -x_t'\phi_{s_t}^{(l+1)})}{\sigma_{(l+1)}^2}P_{\lambda_l}(S_t = s_t,...,S_{t-m} = s_{t-m}|\mathcal{Y}_T = \vec{y}_T) = 0
\\&amp; \Leftrightarrow \sum_{t = m+1}^{T}\sum_{s_t = 1}^{N}\cfrac{(y_t - c_{(l+1)} -x_t'\phi_{s_t}^{(l+1)})}{\sigma_{(l+1)}^2}P_{\lambda_l}(S_t = s_t|\mathcal{Y}_T = \vec{y}_T) = 0
\\&amp; \Leftrightarrow \sum_{t = m+1}^{T}\sum_{s_t = 1}^{N}(y_t - c_{(l+1)} -x_t'\phi_{s_t}^{(l+1)})P_{\lambda_l}(S_t = s_t|\mathcal{Y}_T = \vec{y}_T) = 0
\\&amp; \Leftrightarrow \sum_{t = m+1}^{T}\sum_{s_t = 1}^{N}(y_t - x_t'\phi_{s_t}^{(l+1)})P_{\lambda_l}(S_t = s_t|\mathcal{Y}_T = \vec{y}_T) = \sum_{t = m+1}^{T}\sum_{s_t = 1}^{N}c_{(l+1)}P_{\lambda_l}(S_t = s_t|\mathcal{Y}_T = \vec{y}_T)
\\&amp; \Leftrightarrow \sum_{t = m+1}^{T}\sum_{s_t = 1}^{N}(y_t - x_t'\phi_{s_t}^{(l+1)})P_{\lambda_l}(S_t = s_t|\mathcal{Y}_T = \vec{y}_T) = (T-m)c_{(l+1)}
\\&amp; \Leftrightarrow c_{(l+1)} = \cfrac{1}{(T-m)}\sum_{t = m+1}^{T}\sum_{s_t = 1}^{N}(y_t - x_t'\phi_{s_t}^{(l+1)})P_{\lambda_l}(S_t = s_t|\mathcal{Y}_T = \vec{y}_T).
\end{align*}\]</span> The last result: <span class="math display">\[\begin{equation}
\displaystyle
c_{(l+1)} = \cfrac{1}{(T-m)}\sum_{t = m+1}^{T}\sum_{j = 1}^{N}(y_t - x_t'\phi_{s_t}^{(l+1)})P_{\lambda_l}(S_t = j|\mathcal{Y}_T = \vec{y}_T)
\end{equation}\]</span> can be understood as a constraint. Next, we take the derivative in <span class="math inline">\(\phi_{j}\)</span>: <span class="math display">\[\begin{align*}
\displaystyle
\cfrac{\partial \ln(f_{Y_t|Z_t;\alpha}(y_t|z_t))}{\partial \phi_j} = \cfrac{(y_t - c - x_t'\phi_j)x_t}{\sigma^2}, \quad \text{if $S_t = j$, else $0$}.
\end{align*}\]</span> We insert in (39): <span class="math display">\[\begin{align*}
\displaystyle
&amp;\sum_{t = m+1}^{T}\sum_{s_t = 1}^{N}\cdots\sum_{s_{t-m} = 1}^{N}\cfrac{\partial \ln(f_{Y_t|Z_t;\alpha}(y_t|z_t))}{\partial \phi_j}\Big|_{\alpha = \alpha^{(l+1)}}   P_{\lambda_l}(S_t = s_t,...,S_{t-m} = s_{t-m}|\mathcal{Y}_T = \vec{y}_T) = 0
\\&amp; \Leftrightarrow \sum_{t = m+1}^{T}\cfrac{(y_t - c_{(l+1)} - x_t'\phi_j^{(l+1)})x_t}{\sigma_{(l+1)}^2}P_{\lambda_l}(S_t = j|\mathcal{Y}_T = \vec{y}_T) = 0
\\&amp; \Leftrightarrow \sum_{t = m+1}^{T}(y_t - c_{(l+1)} - x_t'\phi_j^{(l+1)})x_tP_{\lambda_l}(S_t = j|\mathcal{Y}_T = \vec{y}_T) = 0.
\end{align*}\]</span> This is equivalent to the FOC of the following optimization problem: <span class="math display">\[\begin{equation}
\displaystyle
\arg\min_{\phi_j} \sum_{t = m+1}^{T}\left((y_t - c)\sqrt{P_{\lambda_l}(S_t = j|\mathcal{Y}_T = \vec{y}_T)} - x_t'\sqrt{P_{\lambda_l}(S_t = j|\mathcal{Y}_T = \vec{y}_T)}\phi_j\right)^2.
\end{equation}\]</span> Finally, it remains to differentiate with respect to <span class="math inline">\(\sigma^2\)</span>. In this special case, the differentiation proceeds exactly as in Hamilton’s example, since <span class="math inline">\(\sigma^2\)</span> still does not switch. Thus, we have: <span class="math display">\[\begin{align*}
\displaystyle
\sigma_{(l+1)}^2 = \cfrac{1}{(T-M)}\sum_{t = m+1}^T\sum_{j = 1}^{N}(y_t - c_{(l+1)} - x_t'\phi_j^{(l+1)})^2P_{\lambda_l}(S_t = j|\mathcal{Y}_T = \vec{y}_T).
\end{align*}\]</span> It thus becomes apparent that <span class="math inline">\(\sigma^2\)</span> does not affect the conditions for <span class="math inline">\(c\)</span> and <span class="math inline">\(\phi_j\)</span>, while these in turn do influence the condition for <span class="math inline">\(\sigma^2\)</span>. Accordingly, as in Hamilton’s example, one can first solve for <span class="math inline">\(\beta_j\)</span> before determining <span class="math inline">\(\sigma^2\)</span>. However, what changed is that finding <span class="math inline">\(\beta_j\)</span> is no longer a simple optimization step, since two conditions must now be satisfied simultaneously—namely, those for <span class="math inline">\(\phi_j\)</span> and <span class="math inline">\(c\)</span>. It turns out that simultaneously satisfying both conditions is equivalent to optimizing expression (49) subject to the constraint given by (48). We therefore need to solve the following optimisation problem for <span class="math inline">\(\beta_j\)</span>: <span class="math display">\[\begin{align*}
\displaystyle
&amp;\arg\min_{\phi_j} \sum_{t = m+1}^{T}\left((y_t - c)\sqrt{P_{\lambda_l}(S_t = j|\mathcal{Y}_T = \vec{y}_T)} - x_t'\sqrt{P_{\lambda_l}(S_t = j|\mathcal{Y}_T = \vec{y}_T)}\phi_j\right)^2 \quad s.t.
\\&amp;c = \cfrac{1}{(T-m)}\sum_{t = m+1}^{T}\sum_{j = 1}^{N}(y_t - x_t'\phi_{j})P_{\lambda_l}(S_t = j|\mathcal{Y}_T = \vec{y}_T).
\end{align*}\]</span> Now we can use this <span class="math inline">\(\beta_j\)</span>, similarly to how we know it from Hamilton and get: <span class="math display">\[\begin{align*}
\displaystyle
\sigma^2 = \cfrac{1}{(T-M)}\sum_{t = m+1}^T\sum_{j = 1}^{N}(y_t - c - x_t'\phi_j)^2P_{\lambda_l}(S_t = j|\mathcal{Y}_T = \vec{y}_T).
\end{align*}\]</span> This concludes our <span class="math inline">\(\alpha\)</span> vector for the next iteration.</p>
</section>
<section id="example-2-switching-intercept-and-non-switching-coefficients" class="level3">
<h3 class="anchored" data-anchor-id="example-2-switching-intercept-and-non-switching-coefficients">Example 2: Switching Intercept and Non-Switching Coefficients</h3>
<p>This time, we reverse the roles; instead of letting the coefficients switch, now only the intercept switches. Therefore, the assumed process would have the following form: <span class="math display">\[\begin{align*}
    Y_t = c_{s_t} + \phi_{1}Y_{t-1}+...+\phi_{m}Y_{t-m} + U_t; \quad \text{where} \quad U_{t} \overset{i.i.d.}{\sim} N(0,\sigma^2),
\end{align*}\]</span> we could alternatively write: <span class="math display">\[\begin{align*}
Y_t = c_{s_t} + X_t'\phi + U_t \quad \text{with} \quad
X_t =
\begin{pmatrix}
    Y_{t-1}
    \\Y_{t-2}
    \\...
    \\Y_{t-m}
\end{pmatrix}, \quad
\phi_{s_t} =
\begin{pmatrix}
    \phi_{1}
    \\\phi_{2}
    \\...
    \\\phi_{m}
\end{pmatrix} \quad \text{and} \quad
\beta_{s_t} =
\begin{pmatrix}
    c_{s_t}
    \\\phi_{1}
    \\\phi_{2}
    \\...
    \\\phi_{m}
\end{pmatrix}.
\end{align*}\]</span></p>
<p>In this case, we differentiate with respect to <span class="math inline">\(\phi\)</span>, <span class="math inline">\(c_j\)</span>, and <span class="math inline">\(\sigma^2\)</span>. It is important to note that the error term variance still does not switch. For this setup, we can write: <span class="math display">\[\begin{align*}
\displaystyle
\ln(f_{Y_t|Z_t;\alpha}(y_t|z_t)) = \ln(\cfrac{1}{\sqrt{2\pi}\sigma}) - \cfrac{(y_t - c_{s_t} - x_t'\phi)^2}{2\sigma^2}.
\end{align*}\]</span> First we differentiate with respect to <span class="math inline">\(\phi\)</span> and get: <span class="math display">\[\begin{align*}
\displaystyle
&amp;\cfrac{\partial \ln(f_{Y_t|Z_t;\alpha}(y_t|z_t))}{\partial \phi} = \cfrac{(y_t - c_{s_t} -x_t'\phi)x_t}{\sigma^2} \quad \forall s_t \in \{1,...,N\}.
\end{align*}\]</span> We insert in (39), this leads us to: <span class="math display">\[\begin{align*}
\displaystyle
&amp;\sum_{t = m+1}^{T}\sum_{s_t = 1}^{N}\cdots\sum_{s_{t-m} = 1}^{N}\cfrac{(y_t - c_{s_t}^{(l+1)} -x_t'\phi^{(l+1)})x_t}{\sigma_{(l+1)}^2}P_{\lambda_l}(S_t = s_t,...,S_{t-m} = s_{t-m}|\mathcal{Y}_T = \vec{y}_T) = 0
\\&amp; \Leftrightarrow \sum_{t = m+1}^{T}\sum_{s_t = 1}^{N}\cfrac{(y_t - c_{s_t}^{(l+1)} - x_t'\phi^{(l+1)})x_t}{\sigma_{(l+1)}^2}P_{\lambda_l}(S_t = s_t|\mathcal{Y}_T = \vec{y}_T) = 0
\\&amp; \Leftrightarrow \sum_{t = m+1}^{T}\sum_{s_t = 1}^{N}(y_t - c_{s_t}^{(l+1)} - x_t'\phi^{(l+1)})x_tP_{\lambda_l}(S_t = s_t|\mathcal{Y}_T = \vec{y}_T) = 0
\\&amp; \Leftrightarrow \sum_{t = m+1}^{T}\sum_{s_t = 1}^{N}(y_t - c_{s_t}^{(l+1)})x_tP_{\lambda_l}(S_t = s_t|\mathcal{Y}_T = \vec{y}_T) =
\sum_{t = m+1}^{T}\sum_{s_t = 1}^{N}x_t' \phi^{(l+1)} x_t P_{\lambda_l}(S_t = s_t|\mathcal{Y}_T = \vec{y}_T)
\\&amp; \Leftrightarrow \sum_{t = m+1}^{T}\sum_{s_t = 1}^{N}(y_t - c_{s_t}^{(l+1)})x_tP_{\lambda_l}(S_t = s_t|\mathcal{Y}_T = \vec{y}_T) =
\sum_{t = m+1}^{T}\sum_{s_t = 1}^{N}P_{\lambda_l}(S_t = s_t|\mathcal{Y}_T = \vec{y}_T) x_t x_t' \phi^{(l+1)}
\\&amp; \Leftrightarrow \sum_{t = m+1}^{T}\sum_{s_t = 1}^{N}(y_t - c_{s_t}^{(l+1)})x_tP_{\lambda_l}(S_t = s_t|\mathcal{Y}_T = \vec{y}_T) =
\left(\sum_{t = m+1}^{T}\sum_{s_t = 1}^{N}P_{\lambda_l}(S_t = s_t|\mathcal{Y}_T = \vec{y}_T) x_tx_t'\right) \phi^{(l+1)}
\\&amp; \Leftrightarrow \phi^{(l+1)} = \left(\sum_{t = m+1}^{T}x_tx_t'\right)^{-1}\sum_{t = m+1}^{T}\sum_{s_t = 1}^{N}(y_t - c_{s_t}^{(l+1)})x_tP_{\lambda_l}(S_t = s_t|\mathcal{Y}_T = \vec{y}_T).
\end{align*}\]</span> Next we differentiate with respect to <span class="math inline">\(c_j\)</span>: <span class="math display">\[\begin{align*}
\displaystyle
\cfrac{\partial \ln(f_{Y_t|Z_t;\alpha}(y_t|z_t))}{\partial c_j} = \cfrac{(y_t - c_{j} -x_t'\phi)}{\sigma^2} \quad \text{if $S_t = j$, else 0}.
\end{align*}\]</span> Similarily we insert the result in (39): <span class="math display">\[\begin{align*}
\displaystyle
&amp;\sum_{t = m+1}^{T}\sum_{s_t = 1}^{N}\cdots\sum_{s_{t-m} = 1}^{N}\cfrac{\partial \ln(f_{Y_t|Z_t;\alpha}(y_t|z_t))}{\partial c_j}\Big|_{\alpha = \alpha^{(l+1)}}    P_{\lambda_l}(S_t = s_t,...,S_{t-m} = s_{t-m}|\mathcal{Y}_T = \vec{y}_T) = 0
\\&amp; \Leftrightarrow \sum_{t = m+1}^{T}\cfrac{(y_t - c_j^{(l+1)} - x_t'\phi^{(l+1)})}{\sigma_{(l+1)}^2}P_{\lambda_l}(S_t = j|\mathcal{Y}_T = \vec{y}_T) = 0
\\&amp; \Leftrightarrow \sum_{t = m+1}^{T}(y_t - c_j^{(l+1)} - x_t'\phi^{(l+1)})P_{\lambda_l}(S_t = j|\mathcal{Y}_T = \vec{y}_T) = 0.
\end{align*}\]</span> It should be noted that the last equation is the FOC of the following optimization problem: <span class="math display">\[\begin{equation}
\displaystyle
\arg\min_{c_j} \sum_{t = m+1}^{T}\left((y_t - x_t'\phi)\sqrt{P_{\lambda_l}(S_t = j|\mathcal{Y}_T = \vec{y}_T)} - c_j\sqrt{P_{\lambda_l}(S_t = j|\mathcal{Y}_T = \vec{y}_T)}\right)^2.
\end{equation}\]</span> For <span class="math inline">\(\sigma^2\)</span>, the same condition applies as already formulated by Hamilton, because <span class="math inline">\(\sigma^2\)</span> again does not switch. Thus, we are in a very similar situation as in the previous example, because the conditions resulting from the derivative with respect to <span class="math inline">\(c_j\)</span>, as well as the condition resulting from the derivative with respect to <span class="math inline">\(\phi\)</span>, must be satisfied simultaneously and affect the condition for <span class="math inline">\(\sigma^2\)</span>, whereas <span class="math inline">\(\sigma^2\)</span> does not affect the former conditions. Therefore, one can first satisfy the first two conditions for all <span class="math inline">\(j\)</span> in order to obtain <span class="math inline">\(\beta_j\)</span> for all <span class="math inline">\(j\)</span>, and then determine <span class="math inline">\(\sigma^2\)</span> for the next iteration. Furthermore, it follows again that <span class="math inline">\(\beta_j\)</span>, for a given <span class="math inline">\(j\)</span>, can be found by solving an optimization problem with an equality constraint. In order to obtain <span class="math inline">\(\beta_j\)</span>, the following optimization problem must be solved: <span class="math display">\[\begin{align*}
\displaystyle
&amp;\arg\min_{c_j} \sum_{t = m+1}^{T}\left((y_t - x_t'\phi)\sqrt{P_{\lambda_l}(S_t = j|\mathcal{Y}_T = \vec{y}_T)} - c_j\sqrt{P_{\lambda_l}(S_t = j|\mathcal{Y}_T = \vec{y}_T)}\right)^2  \quad s.t.
\\&amp;\phi = \left(\sum_{t = m+1}^{T}x_tx_t'\right)^{-1}\sum_{t = m+1}^{T}\sum_{j = 1}^{N}(y_t - c_j)x_tP_{\lambda_l}(S_t = j|\mathcal{Y}_T = \vec{y}_T).
\end{align*}\]</span> As usual we compute: <span class="math display">\[\begin{align*}
\displaystyle
\sigma^2 = \cfrac{1}{(T-M)}\sum_{t = m+1}^T\sum_{j = 1}^{N}(y_t - c - x_t'\phi_j)^2P_{\lambda_l}(S_t = j|\mathcal{Y}_T = \vec{y}_T),
\end{align*}\]</span> and with that we get out <span class="math inline">\(\alpha\)</span> vector for the next iteration.</p>
</section>
<section id="example-3-all-parameters-switch" class="level3">
<h3 class="anchored" data-anchor-id="example-3-all-parameters-switch">Example 3: All Parameters Switch</h3>
<p>As a third example, we now consider the case in which all parameters are allowed to switch. That is, we now allow not only the coefficients and the intercept to switch, but also the variance of the error term. It is important to note that in setups discussed earlier one could have made stronger assumptions, namely it would have been easier to assume in Example 0 to Example 2, that <span class="math inline">\(S_t\)</span> was independent of <span class="math inline">\(U_{\tau}\)</span> for all <span class="math inline">\(t\)</span> and <span class="math inline">\(\tau\)</span>. Instead we made the weaker assumptions (16) and (17) so that we can now introduce models where the error term variance is allowed to switch, that wouldn’t have been possible with the stronger set of assumptions. That is the reason why all derivations earlier were done with this weaker set of assumptions. That said, we want to point out that we still make the same assumptions as in section 4.1, this is possible due to the weaker set of assumptions, a stronger set of assumptions wouldn’t allow for models like Example 3 and Example 5. Additionally, it should be noted that, in terms of notation, we now again include a <span class="math inline">\(1\)</span> as the first element of <span class="math inline">\(X_t\)</span> to represent the intercept. This leads to the following process formulation: <span class="math display">\[\begin{align*}
    y_t = c_{s_t} + \phi_{1,s_t}Y_{t-1}+...+\phi_{m,s_t}Y_{t-m} + U_t; \quad \text{where} \quad U_{t} \overset{}{\sim} N(0,\sigma_{s_t}^2),
\end{align*}\]</span> we could alternatively write: <span class="math display">\[\begin{align*}
Y_t = X_t'\beta_{s_t} + U_t \quad \text{with} \quad
X_t =
\begin{pmatrix}
      1
    \\Y_{t-1}
    \\Y_{t-2}
    \\...
    \\Y_{t-m}
\end{pmatrix} \quad \text{and} \quad
\beta_{s_t} =
\begin{pmatrix}
    c_{s_t}
    \\\phi_{1,s_t}
    \\\phi_{2,s_t}
    \\...
    \\\phi_{m,s_t}
\end{pmatrix}.
\end{align*}\]</span></p>
<p>As usual we start with the conditional log-likelihood: <span class="math display">\[\begin{align*}
\displaystyle
\ln(f_{Y_t|Z_t;\alpha}(y_t|z_t)) = \ln(\cfrac{1}{\sqrt{2\pi}\sigma_{s_t}}) - \cfrac{(y_t - x_t\beta_{s_t})^2}{2\sigma_{s_t}^2}.
\end{align*}\]</span> We now need to take the derivative once with respect to <span class="math inline">\(\beta_j\)</span> and once with respect to <span class="math inline">\(\sigma_{j}^2\)</span> for a given <span class="math inline">\(j\)</span>. If we first take the derivative with respect to <span class="math inline">\(\beta_j\)</span>, we obtain: <span class="math display">\[\begin{align*}
\displaystyle
\cfrac{\partial \ln(f_{Y_t|Z_t;\alpha}(y_t|z_t))}{\partial \beta_j} = \cfrac{(y_t - x_t'\beta_j)x_t}{\sigma_{j}^2} \quad \text{if $S_t = j$, else $0$}.
\end{align*}\]</span> We now substitute this into (39) and obtain: <span class="math display">\[\begin{align*}
\displaystyle
&amp;\sum_{t = m+1}^{T}\sum_{s_t = 1}^{N}\cdots\sum_{s_{t-m} = 1}^{N}\cfrac{\partial \ln(f_{Y_t|Z_t;\alpha}(y_t|z_t))}{\partial \beta_j}\Big|_{\alpha = \alpha^{(l+1)}}    P_{\lambda_l}(S_t = s_t,...,S_{t-m} = s_{t-m}|\mathcal{Y}_T = \vec{y}_T) = 0
\\&amp; \Leftrightarrow \sum_{t = m+1}^{T}\cfrac{(y_t - x_t'\beta_j^{(l+1)})x_t}{\sigma_{j,(l+1)}^2}P_{\lambda_l}(S_t = j|\mathcal{Y}_T = \vec{y}_T) = 0.
\end{align*}\]</span> We again observe that this corresponds to the FOC of an optimization problem, namely the following optimization problem: <span class="math display">\[\begin{align*}
\arg\min_{\beta_j} \sum_{t = m+1}^{T}\left(\cfrac{y_t}{\sigma_j}\sqrt{P_{\lambda_l}(S_t = j|\mathcal{Y}_T = \vec{y}_T)} - \cfrac{x_t'}{\sigma_j}\sqrt{P_{\lambda_l}(S_t = j|\mathcal{Y}_T = \vec{y}_T)}\beta_j\right)^2.
\end{align*}\]</span> Next we take the derivative with respect to <span class="math inline">\(\sigma_{j}^{-2}\)</span> and get: <span class="math display">\[\begin{align*}
\displaystyle
\cfrac{\partial \ln(f_{Y_t|Z_t;\alpha}(y_t|z_t))}{\partial \sigma_{j}^{-2}} = \cfrac{\sigma_j^{2}}{2} - \cfrac{(y_t - x_t'\beta_j)^2}{2} \quad \text{if $S_t = j$, else $0$}.
\end{align*}\]</span> We substitute the result into (39), this leads to: <span class="math display">\[\begin{align*}
\displaystyle
&amp;\sum_{t = m+1}^{T}\sum_{s_t = 1}^{N}\cdots\sum_{s_{t-m} = 1}^{N}\cfrac{\partial \ln(f_{Y_t|Z_t;\alpha}(y_t|z_t))}{\partial \sigma_{j}^{-2}}\Big|_{\alpha = \alpha^{(l+1)}}    P_{\lambda_l}(S_t = s_t,...,S_{t-m} = s_{t-m}|\mathcal{Y}_T = \vec{y}_T) = 0
\\&amp; \Leftrightarrow \sum_{t = m+1}^{T}\left(\cfrac{\sigma_{j,(l+1)}^{2}}{2} - \cfrac{(y_t - x_t'\beta_j^{(l+1)})^2}{2}\right)P_{\lambda_l}(S_t = j|\mathcal{Y}_T = \vec{y}_T) = 0
\\&amp; \Leftrightarrow \sum_{t = m+1}^{T}(\sigma_{j,(l+1)}^{2} - (y_t - x_t'\beta_j^{(l+1)})^2)P_{\lambda_l}(S_t = j|\mathcal{Y}_T = \vec{y}_T) = 0
\\&amp; \Leftrightarrow \sum_{t = m+1}^{T}\sigma_{j,(l+1)}^{2}P_{\lambda_l}(S_t = j|\mathcal{Y}_T = \vec{y}_T) = \sum_{t = m+1}^{T}(y_t - x_t'\beta_j^{(l+1)})^2P_{\lambda_l}(S_t = j|\mathcal{Y}_T = \vec{y}_T)
\\&amp; \Leftrightarrow \sigma_{j,(l+1)}^{2}\sum_{t = m+1}^{T}P_{\lambda_l}(S_t = j|\mathcal{Y}_T = \vec{y}_T) = \sum_{t = m+1}^{T}(y_t - x_t'\beta_j^{(l+1)})^2P_{\lambda_l}(S_t = j|\mathcal{Y}_T = \vec{y}_T)
\\&amp; \Leftrightarrow \sigma_{j,(l+1)}^{2} = \cfrac{\sum_{t = m+1}^{T}(y_t - x_t'\beta_j^{(l+1)})^2P_{\lambda_l}(S_t = j|\mathcal{Y}_T = \vec{y}_T)}{\sum_{t = m+1}^{T}P_{\lambda_l}(S_t = j|\mathcal{Y}_T = \vec{y}_T)}.
\end{align*}\]</span> As a result, since <span class="math inline">\(\sigma_j\)</span> now affects the condition for <span class="math inline">\(\beta_j\)</span> and vice versa, we once again arrive at a constrained optimization problem, which leads to <span class="math inline">\(\alpha\)</span> of the next iteration. The constrained optimization problem is given by: <span class="math display">\[\begin{align*}
&amp;\arg\min_{\beta_j} \sum_{t = m+1}^{T}\left(\cfrac{y_t}{\sigma_j}\sqrt{P_{\lambda_l}(S_t = j|\mathcal{Y}_T = \vec{y}_T)} - \cfrac{x_t'}{\sigma_j}\sqrt{P_{\lambda_l}(S_t = j|\mathcal{Y}_T = \vec{y}_T)}\beta_j\right)^2 \quad s.t.
\\&amp;\sigma_j = \sqrt{\cfrac{\sum_{t = m+1}^{T}(y_t - x_t'\beta_j)^2P_{\lambda_l}(S_t = j|\mathcal{Y}_T = \vec{y}_T)}{\sum_{t = m+1}^{T}P_{\lambda_l}(S_t = j|\mathcal{Y}_T = \vec{y}_T)}}.
\end{align*}\]</span></p>
</section>
<section id="example-4-arbitrary-subset-switching-of-cphi-and-non-switching-sigma2" class="level3">
<h3 class="anchored" data-anchor-id="example-4-arbitrary-subset-switching-of-cphi-and-non-switching-sigma2">Example 4: Arbitrary Subset-Switching of <span class="math inline">\((c,\phi)\)</span> and Non-Switching <span class="math inline">\(\sigma^2\)</span></h3>
<p>The three previous examples are essentially special cases of the two model formulations that follow. Therefore, the next two examples represent the most general forms of Markov-Switching AR(m) models that will appear in this paper. The model class introduced next assumes that <span class="math inline">\(\sigma^2\)</span> does not switch, and that an arbitrary subset of <span class="math inline">\(\beta\)</span> is allowed to switch. Accordingly, we divide the parameter vector <span class="math inline">\(\beta\)</span> into the switching components, denoted by <span class="math inline">\(\beta^S\)</span>, and the non-switching components, denoted by <span class="math inline">\(\beta^F\)</span>. The underlying process is therefore formulated as follows: <span class="math display">\[\begin{align*}
    Y_t = (X_t^F)'\beta^F + (X_t^S)'\beta_{s_t}^S +U_t; \quad \text{where} \quad U_{t} \overset{i.i.d.}{\sim} N(0,\sigma^2) \quad \text{and} \quad X_t =
\begin{pmatrix}
      1
    \\Y_{t-1}
    \\Y_{t-2}
    \\...
    \\Y_{t-m}
\end{pmatrix}.
\end{align*}\]</span> Thereby <span class="math inline">\(X_t^F\)</span> and <span class="math inline">\(X_t^S\)</span> are defined such that their elements do not overlap and that the elements of both vectors together are the elements of <span class="math inline">\(X_t\)</span>, to put it more formally: Let <span class="math inline">\(I^F, I^S \subset \{1, \dots, m+1\}\)</span> be disjoint index sets such that <span class="math inline">\(I^F \cap I^S = \emptyset\)</span> and <span class="math inline">\(I^F \cup I^S = \{1, \dots, m+1\}\)</span>, where <span class="math inline">\(m+1\)</span> is the number of coefficients plus intercept. Then we define [ X_t^F = ((X_{t})<em>i)</em>{i I^F}, X_t^S = ((X_{t})<em>i)</em>{i I^S}. ] Again we start with the conditional log-likelihood: <span class="math display">\[\begin{align*}
\displaystyle
\ln(f_{Y_t|Z_t;\alpha}(y_t|z_t)) = \ln(\cfrac{1}{\sqrt{2\pi}\sigma}) - \cfrac{(y_t - (x_t^S)'\beta_{s_t}^S - (x_t^F)'\beta^F)^2}{2\sigma^2}.
\end{align*}\]</span> Accordingly, for this model class, we need to take the derivative with respect to <span class="math inline">\(\sigma^2\)</span>, <span class="math inline">\(\beta_j^S\)</span>, and <span class="math inline">\(\beta^F\)</span>. We will start with <span class="math inline">\(\beta_j^S\)</span>: <span class="math display">\[\begin{align*}
\displaystyle
\cfrac{\partial \ln(f_{Y_t|Z_t;\alpha}(y_t|z_t))}{\partial \beta_j^S} = \cfrac{(y_t - (x_t^S)'\beta_j^S - (x_t^F)'\beta^F)x_{t}^{S}}{\sigma^2} \quad \text{if $S_t = j$, else $0$}.
\end{align*}\]</span> We can now substitute this into (39) and obtain: <span class="math display">\[\begin{align*}
\displaystyle
&amp;\sum_{t = m+1}^{T}\sum_{s_t = 1}^{N}\cdots\sum_{s_{t-m} = 1}^{N}\cfrac{\partial \ln(f_{Y_t|Z_t;\alpha}(y_t|z_t))}{\partial \beta_j^S}\Big|_{\alpha = \alpha^{(l+1)}}    P_{\lambda_l}(S_t = s_t,...,S_{t-m} = s_{t-m}|\mathcal{Y}_T = \vec{y}_T) = 0
\\&amp; \Leftrightarrow \sum_{t = m+1}^{T}(y_t - (x_t^S)'\beta_{j,(l+1)}^S - (x_t^F)'\beta_{(l+1)}^F)x_t^S P_{\lambda_l}(S_t = j|\mathcal{Y}_T = \vec{y}_T) = 0.
\end{align*}\]</span> This, in turn, corresponds to the FOC of the following optimization problem: <span class="math display">\[\begin{align*}
\arg\min_{\beta_j^S} \sum_{t = m+1}^{T}\left((y_t - (x_t^F)'\beta^F)\sqrt{P_{\lambda_l}(S_t = j|\mathcal{Y}_T = \vec{y}_T)} - (x_t^S)'\sqrt{P_{\lambda_l}(S_t = j|\mathcal{Y}_T = \vec{y}_T)}\beta_j^S\right)^2.
\end{align*}\]</span> Next, if we take the derivative with respect to <span class="math inline">\(\beta^F\)</span>, we obtain: <span class="math display">\[\begin{align*}
\displaystyle
\cfrac{\partial \ln(f_{Y_t|Z_t;\alpha}(y_t|z_t))}{\partial \beta^F} = \cfrac{(y_t - (x_t^S)'\beta_j^S - (x_t^F)'\beta^F)x_t^F}{\sigma^2} \quad \forall s_t \in \{1,...,N\}.
\end{align*}\]</span> We substitute this into (39) and obtain: <span class="math display">\[\begin{align*}
\displaystyle
&amp;\sum_{t = m+1}^{T}\sum_{s_t = 1}^{N}\cdots\sum_{s_{t-m} = 1}^{N}\cfrac{(y_t - (x_t^S)'\beta_{s_t,(l+1)}^S - (x_t^F)'\beta_{(l+1)}^F)x_t^F}{\sigma_{(l+1)}^2}P_{\lambda_l}(S_t = s_t,...,S_{t-m} = s_{t-m}|\mathcal{Y}_T = \vec{y}_T) = 0
\\&amp; \Leftrightarrow \sum_{t = m+1}^{T}\sum_{s_t = 1}^{N}(y_t - (x_t^S)'\beta_{s_t,(l+1)}^S - (x_t^F)'\beta_{(l+1)}^F)x_t^F P_{\lambda_l}(S_t = s_t|\mathcal{Y}_T = \vec{y}_T) = 0
\\&amp; \Leftrightarrow \sum_{t = m+1}^{T}\sum_{s_t = 1}^{N}(y_t - (x_t^S)'\beta_{s_t,(l+1)}^S)x_t^F P_{\lambda_l}(S_t = s_t|\mathcal{Y}_T = \vec{y}_T) = \sum_{t = m+1}^{T}\sum_{s_t = 1}^{N}(x_t^F)'\beta_{(l+1)}^Fx_t^F P_{\lambda_l}(S_t = s_t|\mathcal{Y}_T = \vec{y}_T)
\\&amp; \Leftrightarrow \sum_{t = m+1}^{T}\sum_{s_t = 1}^{N}(y_t - (x_t^S)'\beta_{s_t,(l+1)}^S)x_t^F P_{\lambda_l}(S_t = s_t|\mathcal{Y}_T = \vec{y}_T) = \sum_{t = m+1}^{T}(x_t^F)'\beta_{(l+1)}^Fx_t^F
\\&amp; \Leftrightarrow \sum_{t = m+1}^{T}\sum_{s_t = 1}^{N}(y_t - (x_t^S)'\beta_{s_t,(l+1)}^S)x_t^F P_{\lambda_l}(S_t = s_t|\mathcal{Y}_T = \vec{y}_T) = \sum_{t = m+1}^{T}x_t^F(x_t^F)'\beta_{(l+1)}^F
\\&amp; \Leftrightarrow \sum_{t = m+1}^{T}\sum_{s_t = 1}^{N}(y_t - (x_t^S)'\beta_{s_t,(l+1)}^S)x_t^F P_{\lambda_l}(s_t = s_t|\mathcal{Y}_T = \vec{y}_T) = \left(\sum_{t = m+1}^{T}x_t^F(x_t^F)'\right)\beta_{(l+1)}^F
\\&amp; \Leftrightarrow \beta_{(l+1)}^F = \left(\sum_{t = m+1}^{T}x_t^F(x_t^F)'\right)^{-1}\left(\sum_{t = m+1}^{T}\sum_{s_t = 1}^{N}(y_t - (x_t^S)'\beta_{s_t,(l+1)}^S)x_t^F P_{\lambda_l}(S_t = s_t|\mathcal{Y}_T = \vec{y}_T)\right).
\end{align*}\]</span> For this generalized model class, where arbitrary parameters can switch except for the error term variance, which can not switch, we thus obtain the following constrained optimization problem to determine <span class="math inline">\(\beta^F\)</span> and <span class="math inline">\(\beta_j^S\)</span> for all <span class="math inline">\(j\)</span>. <span class="math display">\[\begin{align*}
&amp;\arg\min_{\beta_j^S} \sum_{t = m+1}^{T}\left((y_t - (x_t^F)'\beta^F)\sqrt{P_{\lambda_l}(S_t = j|\mathcal{Y}_T = \vec{y}_T)} - (x_t^S)'\sqrt{P_{\lambda_l}(S_t = j|\mathcal{Y}_T = \vec{y}_T)}\beta_j^S\right)^2 \quad s.t.
\\&amp; \beta^F = \left(\sum_{t = m+1}^{T}x_t^F(x_t^F)'\right)^{-1}\left(\sum_{t = m+1}^{T}\sum_{j = 1}^{N}(y_t - (x_t^S)'\beta_j^S)x_t^F P_{\lambda_l}(S_t = j|\mathcal{Y}_T = \vec{y}_T)\right).
\end{align*}\]</span> We then calculate, as usual: <span class="math display">\[\begin{align*}
\displaystyle
\sigma^2 = \cfrac{1}{(T-M)}\sum_{t = m+1}^T\sum_{j = 1}^{N}(y_t - x_t'\beta_j)^2 P_{\lambda_l}(S_t = j|\mathcal{Y}_T = \vec{y}_T).
\end{align*}\]</span> and thus obtain our new <span class="math inline">\(\alpha\)</span> vector for the next iteration.</p>
</section>
<section id="example-5-arbitrary-subset-switching-of-cphi-and-switching-sigma2" class="level3">
<h3 class="anchored" data-anchor-id="example-5-arbitrary-subset-switching-of-cphi-and-switching-sigma2">Example 5: Arbitrary Subset-Switching of <span class="math inline">\((c,\phi)\)</span> and Switching <span class="math inline">\(\sigma^2\)</span></h3>
<p>With Example 5, we complete the generalization of the application of the EM algorithm to underlying AR(m) models, as Example 4 and Example 5 together allow for selecting any arbitrary subset of parameters in an AR(m) context for switching. In this final example, we assume the following underlying process: <span class="math display">\[\begin{align*}
    Y_t = (X_t^F)'\beta^F + (X_t^S)'\beta_{s_t}^S +U_t; \quad \text{where} \quad U_{t} \overset{}{\sim} N(0,\sigma_{s_t}^2) \quad \text{and} \quad X_t =
\begin{pmatrix}
      1
    \\Y_{t-1}
    \\Y_{t-2}
    \\...
    \\Y_{t-m}
\end{pmatrix}.
\end{align*}\]</span> Thereby, <span class="math inline">\(X_t^F\)</span> and <span class="math inline">\(X_t^S\)</span> are defined such that their elements do not overlap and that the elements of both vectors together are the elements of <span class="math inline">\(X_t\)</span>, to put it more formally: Let <span class="math inline">\(I^F, I^S \subset \{1, \dots, m+1\}\)</span> be disjoint index sets such that <span class="math inline">\(I^F \cap I^S = \emptyset\)</span> and <span class="math inline">\(I^F \cup I^S = \{1, \dots, m+1\}\)</span>, where <span class="math inline">\(m+1\)</span> is the number of coefficients plus intercept. Then we define [ X_t^F = ((X_{t})<em>i)</em>{i I^F}, X_t^S = ((X_{t})<em>i)</em>{i I^S}. ] Again, we start with the conditional log-liklihood, which would be in this case: <span class="math display">\[\begin{align*}
\displaystyle
\ln(f_{Y_t|Z_t;\alpha}(y_t|z_t)) = \ln\left(\cfrac{1}{\sqrt{2\pi}\sigma_{s_t}}\right) - \cfrac{(y_t - (x_t^S)'\beta_{s_t}^S - (x_t^F)'\beta^F)^2}{2\sigma_{s_t}^2}.
\end{align*}\]</span> Accordingly, we need to take the derivative with respect to <span class="math inline">\(\sigma_j^2\)</span>, <span class="math inline">\(\beta_j^S\)</span>, and <span class="math inline">\(\beta^F\)</span>. We begin with the derivative with respect to <span class="math inline">\(\beta_j^S\)</span>. <span class="math display">\[\begin{align*}
\displaystyle
\cfrac{\partial \ln(f_{Y_t|Z_t;\alpha}(y_t|z_t))}{\partial \beta_j^S} = \cfrac{(y_t - (x_t^S)'\beta_j^S - (x_t^F)'\beta^F)x_{t}^{S}}{\sigma_j^2} \quad \text{if $S_t = j$, else $0$}.
\end{align*}\]</span> We now substitute this into (39) and obtain: <span class="math display">\[\begin{align*}
\displaystyle
&amp;\sum_{t = m+1}^{T}\sum_{s_t = 1}^{N}\cdots\sum_{s_{t-m} = 1}^{N}\cfrac{\partial \ln(f_{Y_t|Z_t;\alpha}(y_t|z_t))}{\partial \beta_j^S}\Big|_{\alpha = \alpha^{(l+1)}}     P_{\lambda_l}(S_t = s_t,...,S_{t-m} = s_{t-m}|\mathcal{Y}_T = \vec{y}_T) = 0
\\&amp; \Leftrightarrow \sum_{t = m+1}^{T}\cfrac{(y_t - (x_t^S)'\beta_{j,(l+1)}^S - (x_t^F)'\beta_{(l+1)}^F)x_t^S}{\sigma_{j,(l+1)}^2}P_{\lambda_l}(S_t = j|\mathcal{Y}_T = \vec{y}_T) = 0.
\end{align*}\]</span> This, in turn, corresponds to the FOC of the following optimization problem: <span class="math display">\[\begin{align*}
\arg\min_{\beta_j^S} \sum_{t = m+1}^{T}\left((y_t - (x_t^F)'\beta^F)\cfrac{\sqrt{P_{\lambda_l}(S_t = j|\mathcal{Y}_T = \vec{y}_T)}}{\sigma_j} - (x_t^S)'\cfrac{\sqrt{P_{\lambda_l}(S_t = j|\mathcal{Y}_T = \vec{y}_T)}}{\sigma_j}\beta_j^S\right)^2.
\end{align*}\]</span> We now move on to the next step and take the derivative with respect to <span class="math inline">\(\beta^F\)</span>, obtaining: <span class="math display">\[\begin{align*}
\displaystyle
\cfrac{\partial \ln(f_{Y_t|Z_t;\alpha}(y_t|z_t))}{\partial \beta^F} = \cfrac{(y_t - (x_t^S)'\beta_{s_t}^S - (x_t^F)'\beta^F)x_t^F}{\sigma_{s_t}^2} \quad \forall s_t \in \{1,...,N\}.
\end{align*}\]</span> We then substitute this into (39) and obtain: <span class="math display">\[\begin{align*}
\displaystyle
&amp;\sum_{t = m+1}^{T}\sum_{s_t = 1}^{N}\cdots\sum_{s_{t-m} = 1}^{N}\cfrac{(y_t - (x_t^S)'\beta_{s_t,(l+1)}^S - (x_t^F)'\beta_{(l+1)}^F)x_t^F}{\sigma_{s_t,{(l+1)}}^2}P_{\lambda_l}(S_t = s_t,...,S_{t-m} = s_{t-m}|\mathcal{Y}_T = \vec{y}_T) = 0
\\&amp; \Leftrightarrow \sum_{t = m+1}^{T}\sum_{s_t = 1}^{N}\cfrac{(y_t - (x_t^S)'\beta_{s_t,(l+1)}^S - (x_t^F)'\beta_{(l+1)}^F)x_t^F}{\sigma_{s_t,(l+1)}^2}P_{\lambda_l}(S_t = s_t|\mathcal{Y}_T = \vec{y}_T) = 0
\\&amp; \Leftrightarrow \sum_{t = m+1}^{T}\sum_{s_t = 1}^{N}(y_t - (x_t^S)'\beta_{s_t,(l+1)}^S)x_t^F\cfrac{P_{\lambda_l}(S_t = s_t|\mathcal{Y}_T = \vec{y}_T)}{\sigma_{s_t,(l+1)}^2} = \sum_{t = m+1}^{T}\sum_{s_t = 1}^{N}(x_t^F)'\beta_{(l+1)}^Fx_t^F\cfrac{P_{\lambda_l}(S_t = s_t|\mathcal{Y}_T = \vec{y}_T)}{\sigma_{s_t,(l+1)}^2}
\\&amp; \Leftrightarrow \sum_{t = m+1}^{T}\sum_{s_t = 1}^{N}(y_t - (x_t^S)'\beta_{s_t,(l+1)}^S)x_t^F\cfrac{P_{\lambda_l}(S_t = s_t|\mathcal{Y}_T = \vec{y}_T)}{\sigma_{s_t,(l+1)}^2} = \sum_{t = m+1}^{T}(x_t^F)'\beta_{(l+1)}^Fx_t^F\sum_{s_t = 1}^{N}\cfrac{P_{\lambda_l}(S_t = s_t|\mathcal{Y}_T = \vec{y}_T)}{\sigma_{s_t,(l+1)}^2}
\\&amp; \Leftrightarrow \sum_{t = m+1}^{T}\sum_{s_t = 1}^{N}(y_t - (x_t^S)'\beta_{s_t,(l+1)}^S)x_t^F\cfrac{P_{\lambda_l}(S_t = s_t|\mathcal{Y}_T = \vec{y}_T)}{\sigma_{s_t,(l+1)}^2} = \sum_{t = m+1}^{T}x_t^F(x_t^F)'\beta_{(l+1)}^F\sum_{s_t = 1}^{N}\cfrac{P_{\lambda_l}(S_t = s_t|\mathcal{Y}_T = \vec{y}_T)}{\sigma_{s_t,(l+1)}^2}
\\&amp; \Leftrightarrow \sum_{t = m+1}^{T}\sum_{s_t = 1}^{N}(y_t - (x_t^S)'\beta_{s_t,(l+1)}^S)x_t^F\cfrac{P_{\lambda_l}(S_t = s_t|\mathcal{Y}_T = \vec{y}_T)}{\sigma_{s_t,(l+1)}^2} = \sum_{t = m+1}^{T}x_t^F(x_t^F)'\sum_{s_t = 1}^{N}\cfrac{P_{\lambda_l}(S_t = s_t|\mathcal{Y}_T = \vec{y}_T)}{\sigma_{s_t,(l+1)}^2}\beta_{(l+1)}^F
\\&amp; \Leftrightarrow \sum_{t = m+1}^{T}\sum_{s_t = 1}^{N}(y_t - (x_t^S)'\beta_{s_t,(l+1)}^S)x_t^F\cfrac{P_{\lambda_l}(S_t = s_t|\mathcal{Y}_T = \vec{y}_T)}{\sigma_{s_t,(l+1)}^2} = \left(\sum_{t = m+1}^{T}x_t^F(x_t^F)'\sum_{s_t = 1}^{N}\cfrac{P_{\lambda_l}(S_t = s_t|\mathcal{Y}_T = \vec{y}_T)}{\sigma_{s_t,(l+1)}^2}\right)\beta_{(l+1)}^F
\\&amp; \Leftrightarrow \beta_{(l+1)}^F = \left(\sum_{t = m+1}^{T}x_t^F(x_t^F)'\sum_{s_t = 1}^{N}\cfrac{P_{\lambda_l}(S_t = s_t|\mathcal{Y}_T = \vec{y}_T)}{\sigma_{s_t,(l+1)}^2}\right)^{-1}\left(\sum_{t = m+1}^{T}\sum_{s_t = 1}^{N}(y_t - (x_t^S)'\beta_{s_t,(l+1)}^S)x_t^F\cfrac{P_{\lambda_l}(S_t = s_t|\mathcal{Y}_T = \vec{y}_T)}{\sigma_{s_t,(l+1)}^2}\right).
\end{align*}\]</span> As a third step, we now take the derivative with respect to <span class="math inline">\(\sigma_j^{-2}\)</span> and obtain: <span class="math display">\[\begin{align*}
\displaystyle
\cfrac{\partial \ln(f_{Y_t|Z_t;\alpha}(y_t|z_t))}{\partial \sigma_j^{-2}} = \cfrac{\sigma_j^2}{2} - \cfrac{(y_t - (x_t^S)'\beta_j^S - (x_t^F)'\beta^F)^2}{2} \quad \text{if $S_t = j$ else $0$}.
\end{align*}\]</span> We now substitute this into (39) and obtain: <span class="math display">\[\begin{align*}
\displaystyle
&amp;\sum_{t = m+1}^{T}\sum_{s_t = 1}^{N}\cdots\sum_{s_{t-m} = 1}^{N}\cfrac{\partial \ln(f_{Y_t|Z_t;\alpha}(y_t|z_t))}{\partial \sigma_j^{-2}}\Big|_{\alpha = \alpha^{(l+1)}}      P_{\lambda_l}(S_t = s_t,...,S_{t-m} = s_{t-m}|\mathcal{Y}_T = \vec{y}_T) = 0
\\&amp; \Leftrightarrow \sum_{t = m+1}^{T}\left(\cfrac{\sigma_{j,(l+1)}^2}{2} - \cfrac{(y_t - (x_t^S)'\beta_{j,(l+1)}^S - (x_t^F)'\beta_{(l+1)}^F)^2}{2} \right)P_{\lambda_l}(S_t = j|\mathcal{Y}_T = \vec{y}_T) = 0
\\&amp; \Leftrightarrow \sum_{t = m+1}^{T}\sigma_{j,(l+1)}^2 P_{\lambda_l}(S_t = j|\mathcal{Y}_T = \vec{y}_T) = \sum_{t = m+1}^{T}(y_t - (x_t^S)'\beta_{j,(l+1)}^S - (x_t^F)'\beta_{(l+1)}^F)^2 P_{\lambda_l}(S_t = j|\mathcal{Y}_T = \vec{y}_T)
\\&amp; \Leftrightarrow \sigma_{j,(l+1)}^2\sum_{t = m+1}^{T} P_{\lambda_l}(S_t = j|\mathcal{Y}_T = \vec{y}_T) = \sum_{t = m+1}^{T}(y_t - (x_t^S)'\beta_{j,s_t}^S - (x_t^F)'\beta_{(l+1)}^F)^2 P_{\lambda_l}(S_t = j|\mathcal{Y}_T = \vec{y}_T)
\\&amp; \Leftrightarrow \sigma_{j,(l+1)}^2 = \cfrac{\sum_{t = m+1}^{T}(y_t - (x_t^S)'\beta_{j,s_t}^S - (x_t^F)'\beta_{(l+1)}^F)^2 P_{\lambda_l}(S_t = j|\mathcal{Y}_T = \vec{y}_T)}{\sum_{t = m+1}^{T} P_{\lambda_l}(S_t = j|\mathcal{Y}_T = \vec{y}_T)}.
\end{align*}\]</span> We can now combine these three results into a single optimization problem, which leads us to the next <span class="math inline">\(\alpha\)</span>. This is necessary because, once again, all three conditions must be satisfied simultaneously and cannot be implemented sequentially, as each of the three variables plays a role in the different conditions. We thus obtain another constrained optimization problem, but this time under two constraints: <span class="math display">\[\begin{align*}
&amp;\arg\min_{\beta_j^S} \sum_{t = m+1}^{T}\left((y_t - (x_t^F)'\beta^F)\cfrac{\sqrt{P_{\lambda_l}(S_t = j|\mathcal{Y}_T = \vec{y}_T)}}{\sigma_j} - (x_t^S)'\cfrac{\sqrt{P_{\lambda_l}(S_t = j|\mathcal{Y}_T = \vec{y}_T)}}{\sigma_j}\beta_j^S\right)^2 \quad s.t.\\
&amp;\beta^F = \left(\sum_{t = m+1}^{T}x_t^F(x_t^F)'\sum_{j = 1}^{N}\cfrac{P_{\lambda_l}(S_t = j|\mathcal{Y}_T = \vec{y}_T)}{\sigma_j^2}\right)^{-1}\left(\sum_{t = m+1}^{T}\sum_{j = 1}^{N}(y_t - (x_t^S)'\beta_j^S)x_t^F\cfrac{P_{\lambda_l}(S_t = j|\mathcal{Y}_T = \vec{y}_T)}{\sigma_j^2}\right) \\
&amp;\sigma_j = \sqrt{\cfrac{\sum_{t = m+1}^{T}(y_t - (x_t^S)'\beta_j^S - (x_t^F)'\beta^F)^2 P_{\lambda_l}(S_t = j|\mathcal{Y}_T = \vec{y}_T)}{\sum_{t = m+1}^{T} P_{\lambda_l}(s_t = j|\mathcal{Y}_T = \vec{y}_T)}}.
\end{align*}\]</span></p>
</section>
</section>
<section id="forecasting-with-markov-switching-models" class="level2">
<h2 class="anchored" data-anchor-id="forecasting-with-markov-switching-models">Forecasting with Markov-Switching Models</h2>
<p>Next, we would like to turn to the topic of forecasts using Markov-Switching models. We recall that the conditional density is given by <span class="math inline">\(f_{Y_t|S_t,\mathcal{Y}_{t-1}; \alpha}(y_t|j,\vec{y}_{t-1})\)</span>. If we now have <span class="math inline">\(\vec{y}_t\)</span> and <span class="math inline">\(s_{t+1}\)</span>, we could, for a simple AR(1) model, state: <span class="math display">\[\begin{align*}
Y_{t+1} = c_{s_{t+1}} + \phi_{s_{t+1}}Y_t + U_{t+1},
\end{align*}\]</span> where we have <span class="math inline">\(E_{\alpha}(Y_{t+1}|S_{t+1} = j, \mathcal{Y}_t = \vec{y}_t) = c_j + \phi_j y_t\)</span>. Furthermore, we can show the following for an m-step ahead forecast: <span class="math display">\[\begin{align*}
\displaystyle
E_{\theta}(Y_{t+m}|\mathcal{Y}_t = \vec{y}_t)
&amp;= \int y_{t+m}f_{Y_{t+m}|\mathcal{Y}_t;\theta}(y_{t+m}|\vec{y}_t) \,dy_{t+m}
\\ &amp;= \int y_{t+m}\left(\sum_{j = 1}^{N}f_{Y_{t+m},S_{t+m}|\mathcal{Y}_T;\theta}(y_{t+m},j|\vec{y}_t)\right) \,dy_{t+m}
\\ &amp;= \int y_{t+m}\left(\sum_{j = 1}^{N}f_{Y_{t+1}|S_{t+m},\mathcal{Y}_t;\alpha}(y_{t+m}|j,\vec{y}_t)P_{\theta}(S_{t+m} = j|\mathcal{Y}_t = \vec{y}_t)\right) \,dy_{t+m}
\\ &amp;= \sum_{j = 1}^{N}P_{\theta}(S_{t+m} = j|\mathcal{Y}_t = \vec{y}_t)
\int y_{t+m}f_{Y_{t+m}|S_{t+m},\mathcal{Y}_t;\alpha}(y_{t+m}|j,\vec{y}_t) \,dy_{t+m}
\\ &amp;= \sum_{j = 1}^{N}P_{\theta}(S_{t+m} = j|\mathcal{Y}_t = \vec{y}_t)
E_{\alpha}(Y_{t+m}|S_{t+m} = j,\mathcal{Y}_t = \vec{y}_t).
\end{align*}\]</span> One could thus say that the forecasts for <span class="math inline">\(y_{t+m}\)</span> correspond to a weighted average of the expected values given the regime, with the regime probabilities as the weights. To summarize this notation, we can say that we collect the <span class="math inline">\(E_{\alpha}(Y_{t+m}|S_{t+m} = j, \mathcal{Y}_t = \vec{y}_t)\)</span> in <span class="math inline">\(\mathbf{h_t'}\)</span>, so that we can write <span class="math inline">\(E_{\theta}(Y_{t+m}|\mathcal{Y}_t = \vec{y}_t) = \mathbf{h_t'} \hat{\zeta}_{t+m|t}\)</span>, this derivation closely followed Hamilton (1994, page 694-695).</p>
</section>
<section id="regime-forecasting-with-markov-switching-models" class="level2">
<h2 class="anchored" data-anchor-id="regime-forecasting-with-markov-switching-models">Regime Forecasting with Markov-Switching Models</h2>
<p>The probability that the Markov-Chain will be in a particular state in the future can be considered as <span class="math inline">\(E_{\theta}(\zeta_{t+m}|\mathcal{Y}_t = \vec{y}_t)\)</span>. Based on (14), this is given by: <span class="math display">\[\begin{equation}
E_{\theta}(\zeta_{t+m}|\mathcal{Y}_t = \vec{y}_t) = (\Pi')^mE_{\theta}(\zeta_t|\mathcal{Y}_t = \vec{y}_t),
\end{equation}\]</span> or alternatively: <span class="math display">\[\begin{equation}
\hat{\zeta}_{t+m|t} = (\Pi')^m \hat{\zeta}_{t|t}.
\end{equation}\]</span></p>
</section>
</section>
<section id="appendix" class="level1">
<h1>Appendix</h1>
<section id="optimal-inference-of-the-regimes-and-derivation-of-the-log-likelihood-1" class="level2">
<h2 class="anchored" data-anchor-id="optimal-inference-of-the-regimes-and-derivation-of-the-log-likelihood-1">Optimal Inference of the Regimes and Derivation of the Log-Likelihood</h2>
<p>The following derivation closely follows Hamilton (1994, page 693). First of all, it is essential to keep in mind that <span class="math inline">\((\hat{\zeta}_{t|t-1})_j = P_{\theta}(S_t = j|\mathcal{Y}_{t-1} = \vec{y}_{t-1})\)</span> and <span class="math inline">\((\eta_t)_j = f_{Y_t|S_t, \mathcal{Y}_{t-1};\alpha}(y_t|j,\vec{y}_{t-1})\)</span>. Based on this we can see that: <span class="math display">\[\begin{align*}
\displaystyle
    (\hat{\zeta}_{t|t-1} \odot \eta_t)_j &amp;= P_{\theta}(S_t = j|\mathcal{Y}_{t-1} = \vec{y}_{t-1})f_{Y_t|S_t,\mathcal{Y}_{t-1};\alpha}(y_t|j,\vec{y}_{t-1})
    \\&amp;= f_{Y_t,S_t|\mathcal{Y}_{t-1};\theta}(y_t,j|\vec{y}_{t-1}).
\end{align*}\]</span> If we now sum over all potential values of <span class="math inline">\(S_t\)</span> we get: <span class="math display">\[\begin{align*}
       \sum_{j = 1}^{N}f_{Y_t,S_t|\mathcal{Y}_{t-1};\theta}(y_t,j|\vec{y}_{t-1}) = f_{Y_t|\mathcal{Y}_{t-1};\theta}(y_t|\vec{y}_{t-1}) = \mathbf{1'}(\hat{\zeta}_{t|t-1} \odot \eta_t).
\end{align*}\]</span> Additionally it is therefore true that: <span class="math display">\[\begin{align*}
    \cfrac{(\hat{\zeta}_{t|t-1} \odot \eta_t)_j}{\mathbf{1'}(\hat{\zeta}_{t|t-1} \odot \eta_t)} = \cfrac{f_{Y_t,S_t|\mathcal{Y}_{t-1};\theta}(y_t,j|\vec{y}_{t-1})}{f_{Y_t|\mathcal{Y}_{t-1};\theta}(y_t|\vec{y}_{t-1})} = P_{\theta}(S_t = j|\mathcal{Y}_t = \vec{y}_{t}) = (\hat{\zeta}_{t|t})_j.
\end{align*}\]</span> Utilizing vectors we can write: <span class="math display">\[\begin{align*}
\hat{\zeta}_{t|t} = \cfrac{(\hat{\zeta}_{t|t-1} \odot \eta_t)}{\mathbf{1'}(\hat{\zeta}_{t|t-1} \odot \eta_t)}.
\end{align*}\]</span> To put it as simple as possible one could say that we basically just applied Bayes Rule. Next we want to get from <span class="math inline">\((\hat{\zeta}_{t|t})_j = P_{\theta}(S_t = j|\mathcal{Y}_t = \vec{y}_{t})\)</span> to <span class="math inline">\((\hat{\zeta}_{t+1|t})_j = P_{\theta}(S_{t+1} = j|\mathcal{Y}_t = \vec{y}_{t})\)</span>. We know from (14) that this is possible via: <span class="math display">\[\begin{align*}
    E_{\theta}(\zeta_{t+1}|\mathcal{Y}_t = \vec{y}_t) = \Pi'\hat{\zeta}_{t|t}.
\end{align*}\]</span></p>
</section>
<section id="smoothed-inference-over-the-regimes-1" class="level2">
<h2 class="anchored" data-anchor-id="smoothed-inference-over-the-regimes-1">Smoothed Inference over the Regimes</h2>
<p>The following derivation closely follows Hamilton (1994, page 700-702). First we note that <span class="math inline">\(S_t\)</span> depends on <span class="math inline">\(\mathcal{Y}_{t-1}\)</span> through <span class="math inline">\(S_{t-1}\)</span> and on future observations only through <span class="math inline">\(S_{t+1}\)</span>! One could say: <span class="math display">\[\begin{align*}
P_{\theta}(S_t = j|S_{t+1} = i,\mathcal{Y}_T = \vec{y}_{T}) = P_{\theta}(S_t = j|S_{t+1} = i,\mathcal{Y}_t = \vec{y}_{t}).
\end{align*}\]</span></p>
<p>We can show this formally: <span class="math display">\[\begin{align*}
\displaystyle
P_{\theta}(S_t = j|S_{t+1} = i,\mathcal{Y}_{t+1} = \vec{y}_{t+1})
&amp;= P_{\theta}(S_t = j|S_{t+1} = i,Y_{t+1} = y_{t+1},\mathcal{Y}_{t} = \vec{y}_{t})
\\ &amp;= \cfrac{P_{\theta}(S_t = j,Y_{t+1} = y_{t+1}|S_{t+1} = i,\mathcal{Y}_{t} = \vec{y}_{t})}{f_{Y_{t+1}|S_{t+1},\mathcal{Y}_t;\alpha}(y_{t+1}|i,\vec{y}_{t})}
\\ &amp;= \cfrac{f_{Y_{t+1}|S_t,S_{t+1},\mathcal{Y}_{t};\alpha}(y_{t+1}|j,i,\vec{y}_{t})}{f_{Y_{t+1}|S_{t+1},\mathcal{Y}_t;\alpha}(y_{t+1}|i,\vec{y}_{t})}
P_{\theta}(S_t = j|S_{t+1} = i,\mathcal{Y}_{t} = \vec{y}_{t})
\\ &amp;= \cfrac{f_{Y_{t+1}|S_{t+1},\mathcal{Y}_t;\alpha}(y_{t+1}|i,\vec{y}_{t})}{f_{Y_{t+1}|S_{t+1},\mathcal{Y}_t;\alpha}(y_{t+1}|i,\vec{y}_{t})}
P_{\theta}(S_t = j|S_{t+1} = i,\mathcal{Y}_{t} = \vec{y}_{t})
\\ &amp;= P_{\theta}(S_t = j|S_{t+1} = i,\mathcal{Y}_{t} = \vec{y}_{t}).
\end{align*}\]</span> The last two steps are possible because based on (11) the distribution of <span class="math inline">\(Y_{t+1}\)</span> conditional on <span class="math inline">\(S_{t+1}\)</span> is independent of <span class="math inline">\(S_{t}\)</span>. Now we can approach the derivation for <span class="math inline">\(t+2\)</span> in a similar way: <span class="math display">\[\begin{align*}
\displaystyle
P_{\theta}(S_t = j|S_{t+1} = i,\mathcal{Y}_{t+2} = \vec{y}_{t+2})
&amp;= P_{\theta}(S_t = j|S_{t+1} = i,Y_{t+2} = y_{t+2},\mathcal{Y}_{t+1} = \vec{y}_{t+1})
\\ &amp; = \cfrac{P_{\theta}(S_t = j,Y_{t+2} = y_{t+2}|S_{t+1} = i,\mathcal{Y}_{t+1} = \vec{y}_{t+1}) }{f_{Y_{t+2}|S_{t+1},\mathcal{Y}_{t+1};\theta}(y_{t+2}|i,\vec{y}_{t+1})}
\\ &amp;= \cfrac{f_{Y_{t+2}|S_t,S_{t+1},\mathcal{Y}_{t+1};\theta}(y_{t+2}|j,i,\vec{y}_{t+1})}{f_{Y_{t+2}|S_{t+1},\mathcal{Y}_{t+1};\theta}(y_{t+2}|i,\vec{y}_{t+1})}
P_{\theta}(S_t = j|S_{t+1} = i,\mathcal{Y}_{t+1} = \vec{y}_{t+1})
\\ &amp;= \cfrac{f_{Y_{t+2}|S_{t+1},\mathcal{Y}_{t+1};\theta}(y_{t+2}|i,\vec{y}_{t+1})}{f_{Y_{t+2}|S_{t+1},\mathcal{Y}_{t+1};\theta}(y_{t+2}|i,\vec{y}_{t+1})}
P_{\theta}(S_t = j|S_{t+1} = i,\mathcal{Y}_{t+1} = \vec{y}_{t+1})
\\ &amp;= P_{\theta}(S_t = j|S_{t+1} = i,\mathcal{Y}_{t+1} = \vec{y}_{t+1})
\\ &amp;= P_{\theta}(S_t = j|S_{t+1} = i,\mathcal{Y}_{t} = \vec{y}_{t}).
\end{align*}\]</span> Simplifying the fraction is possible because: <span class="math display">\[\begin{align*}
\displaystyle
f_{Y_{t+2}|S_t,S_{t+1},\mathcal{Y}_{t+1};\theta}(y_{t+2}|j,i,\vec{y}_{t+1})  &amp;= \sum_{k = 1}^{N}f_{Y_{t+2},S_{t+2}|S_t, S_{t+1}, \mathcal{Y}_{t+1};\theta}(y_{t+2},k|j,i,\vec{y}_{t+1})
\\ &amp;= \sum_{k = 1}^{N}f_{Y_{t+2}|S_{t+2},S_t, S_{t+1}, \mathcal{Y}_{t+1};\alpha}(y_{t+2}|k,j,i,\vec{y}_{t+1})
\\ &amp; \hspace{0.5cm} \cdot P_{\theta}(S_{t+2}=k|S_{t+1} = i, S_{t} = j,\mathcal{Y}_{t+1} = \vec{y}_{t+1})
\\ &amp;= \sum_{k = 1}^{N}f_{Y_{t+2}|S_{t+2}, S_{t+1}, \mathcal{Y}_{t+1};\alpha}(y_{t+2}|k,i,\vec{y}_{t+1})
\\ &amp; \hspace{0.5cm} \cdot P_{\theta}(S_{t+2}=k|S_{t+1} = i,\mathcal{Y}_{t+1} = \vec{y}_{t+1})
\\ &amp;= \sum_{k = 1}^{N}f_{Y_{t+2},S_{t+2}|S_{t+1},\mathcal{Y}_{t+1};\theta}(y_{t+2},k|i,\vec{y}_{t+1})
\\ &amp;= f_{Y_{t+2}|S_{t+1}, \mathcal{Y}_{t+1};\theta}(y_{t+2}|i,\vec{y}_{t+1}).
\end{align*}\]</span> We show now by induction that this approach is generally applicable. The earlier shown cases were the start of the induction, for the induction step we can say, we choose an arbitrary, but feasible, <span class="math inline">\(n\)</span> and our induction assumption is: <span class="math display">\[\begin{equation}
    P_{\theta}(S_t = j|S_{t+1} = i,\mathcal{Y}_{t+n} = \vec{y}_{t+n}) = P_{\theta}(S_t = j|S_{t+1} = i,\mathcal{Y}_t = \vec{y}_{t}).
\end{equation}\]</span> Then it shall hold that: <span class="math display">\[\begin{equation}
    P_{\theta}(S_t = j|S_{t+1} = i,\mathcal{Y}_{t+n+1} = \vec{y}_{t+n+1}) = P_{\theta}(S_t = j|S_{t+1} = i,\mathcal{Y}_t = \vec{y}_{t}).
\end{equation}\]</span> To show that we write: <span class="math display">\[\begin{align*}
P_{\theta}(S_t = j|S_{t+1} = i,\mathcal{Y}_{t+n+1} = \vec{y}_{t+n+1}) &amp;= P_{\theta}(S_t =j|S_{t+1}=i,Y_{t+n+1} = y_{t+n+1},\mathcal{Y}_{t+n} = \vec{y}_{t+n})
\\ &amp;= \cfrac{f_{S_t, Y_{t+n+1}|S_{t+1}, \mathcal{Y}_{t+n};\theta}(j,y_{t+n+1}|i,\vec{y}_{t+n})}{f_{Y_{t+n+1}|S_{t+1},\mathcal{Y}_{t+n};\theta}(y_{t+n+1}|i,\vec{y}_{t+n})}
\\ &amp;= \cfrac{f_{Y_{t+n+1}|S_t,S_{t+1},\mathcal{Y}_{t+n};\theta}(y_{t+n+1}|j,i,\vec{y}_{t+n})}{f_{Y_{t+n+1}|S_{t+1},\mathcal{Y}_{t+n};\theta}(y_{t+n+1}|i,\vec{y}_{t+n})}
\\&amp; \hspace{0.5cm} \cdot P_{\theta}(S_t = j|S_{t+1} = i,\mathcal{Y}_{t+n} = \vec{y}_{t+n})
\\ &amp;= P_{\theta}(S_t = j|S_{t+1} = i,\mathcal{Y}_{t+n} = \vec{y}_{t+n})
\\ &amp;= P_{\theta}(S_t = j|S_{t+1} = i,\mathcal{Y}_t = \vec{y}_{t}).
\end{align*}\]</span> Thereby, the last step is just applying the induction assumption and simplifying the fraction is possible because: <span class="math display">\[\begin{align*}
f_{Y_{t+n+1}|S_t, S_{t+1}, \mathcal{Y}_{t+n};\theta}(y_{t+n+1}|j,i,\vec{y}_{t+n}) &amp;= \sum_{k = 1}^{N}f_{Y_{t+n+1},S_{t+n+1}|S_t, S_{t+1}, \mathcal{Y}_{t+n};\theta}(y_{t+n+1},k|j,i, \vec{y}_{t+n})
\\ &amp;= \sum_{k = 1}^{N}f_{Y_{t+n+1}|S_{t+n+1}, S_t, S_{t+1},\mathcal{Y}_{t+n};\alpha}(y_{t+n+1}|k,j,i,\vec{y}_{t+n})
\\&amp; \hspace{0.5cm} \cdot P_{\theta}(S_{t+n+1} = k|S_t = j, S_{t+1} = i, \mathcal{Y}_{t+n} = \vec{y}_{t+n})
\\ &amp;= \sum_{k = 1}^{N}f_{Y_{t+n+1}|S_{t+n+1}, S_{t+1},\mathcal{Y}_{t+n};\alpha}(y_{t+n+1}|k,i,\vec{y}_{t+n})
\\&amp; \hspace{0.5cm} \cdot P_{\theta}(S_{t+n+1} = k| S_{t+1} = i, \mathcal{Y}_{t+n} = \vec{y}_{t+n})
\\ &amp;= \sum_{k = 1}^{N}f_{Y_{t+n+1},S_{t+n+1}|S_{t+1},\mathcal{Y}_{t+n};\theta}(y_{t+n+1},k|i,\vec{y}_{t+n})
\\ &amp;= f_{Y_{t+n+1}|S_{t+1}, \mathcal{Y}_{t+n};\theta}(y_{t+n+1}|i,\vec{y}_{t+n}).
\end{align*}\]</span> Once this is done it can be seen that: <span class="math display">\[\begin{align*}
P_{\theta}(S_{t} = j|S_{t+1} = i,\mathcal{Y}_{t+m} = \vec{y}_{t+m}) = P_{\theta}(S_{t} = j|S_{t+1} = i,\mathcal{Y}_{t} = \vec{y}_{t}).
\end{align*}\]</span> From this follows the original claim: <span class="math display">\[\begin{align*}
P_{\theta}(S_{t} =j|S_{t+1}=i,\mathcal{Y}_{T} = \vec{y}_{T}) = P_{\theta}(S_{t} = j|S_{t+1} = i, \mathcal{Y}_{t} = \vec{y}_{t}).
\end{align*}\]</span> Next we can see that: <span class="math display">\[\begin{align*}
\displaystyle
P_{\theta}(S_{t} =j|S_{t+1} = i,\mathcal{Y}_{t} = \vec{y}_{t})
&amp;= \cfrac{P_{\theta}(S_{t}=j,S_{t+1} = i|\mathcal{Y}_{t} = \vec{y}_{t})}{P_{\theta}(S_{t+1} = i|\mathcal{Y}_{t} = \vec{y}_{t})}
\\ &amp;= \cfrac{P_{\theta}(S_{t+1} = i|S_{t}=j,\mathcal{Y}_{t} = \vec{y}_{t})P_{\theta}(S_{t}=j|\mathcal{Y}_{t} = \vec{y}_{t})}{P_{\theta}(S_{t+1} = i|\mathcal{Y}_{t} = \vec{y}_{t})}
\\ &amp;= \cfrac{P_{\theta}(S_{t+1} = i|S_{t}=j)P_{\theta}(S_{t}=j|\mathcal{Y}_{t} = \vec{y}_{t})}{P_{\theta}(S_{t+1} = i|\mathcal{Y}_{t} = \vec{y}_{t})}
\\ &amp;= \cfrac{\pi_{j,i}P_{\theta}(S_{t}=j|\mathcal{Y}_{t} = \vec{y}_t)}{P_{\theta}(S_{t+1} = i|\mathcal{Y}_{t} = \vec{y}_{t})}.
\end{align*}\]</span> From this it follows that: <span class="math display">\[\begin{align*}
\displaystyle
P_{\theta}(S_{t} = j,S_{t+1} =i|\mathcal{Y}_{T} = \vec{y}_{T})
&amp;= P_{\theta}(S_{t+1} = i|\mathcal{Y}_{T} = \vec{y}_{T})P_{\theta}(S_{t} = j|S_{t+1} = i,\mathcal{Y}_{T}  =\vec{y}_{T})
\\ &amp;= P_{\theta}(S_{t+1} = i|\mathcal{Y}_{T} = \vec{y}_{T})P_{\theta}(S_{t} = j|S_{t+1} = i,\mathcal{Y}_{t} = \vec{y}_{t})
\\ &amp;=  P_{\theta}(S_{t+1} = i|\mathcal{Y}_{T} = \vec{y}_{T})\cfrac{\pi_{j,i}P_{\theta}(S_{t}=j|\mathcal{Y}_{t} = \vec{y}_{t})}{P_{\theta}(S_{t+1} = i|\mathcal{Y}_{t} = \vec{y}_{t})}.
\end{align*}\]</span> Therefore, the smoothed inference over <span class="math inline">\(S_t\)</span> is given by: <span class="math display">\[\begin{align*}
\displaystyle
P_{\theta}(S_{t} = j|\mathcal{Y}_{T} = \vec{y}_{T})
&amp;= \sum_{i = 1}^{N}P_{\theta}(S_t =j,S_{t+1} = i|\mathcal{Y}_{T} = \vec{y}_{T})
\\ &amp;= \sum_{i = 1}^{N}P_{\theta}(S_{t+1} = i|\mathcal{Y}_{T} = \vec{y}_{T})\cfrac{\pi_{j,i}P_{\theta}(S_{t}=j|\mathcal{Y}_{t} = \vec{y}_{t})}{P_{\theta}(S_{t+1} = i|\mathcal{Y}_{t} = \vec{y}_{t})}
\\ &amp;= P_{\theta}(S_t = j|\mathcal{Y}_{t} = \vec{y}_{t})\sum_{i = 1}^{N}\cfrac{\pi_{j,i}P_{\theta}(S_{t+1}=i|\mathcal{Y}_{T} = \vec{y}_{T})}{P_{\theta}(S_{t+1} = i|\mathcal{Y}_{t} = \vec{y}_{t})}
\\ &amp;= P_{\theta}(S_t = j|\mathcal{Y}_{t} = \vec{y}_{t})(\pi_{j,1} ... \pi_{j,N})\begin{pmatrix}
\cfrac{P_{\theta}(S_{t+1} = 1|\mathcal{Y}_{T} = \vec{y}_{T})}{P_{\theta}(S_{t+1} = 1|\mathcal{Y}_{t} = \vec{y}_{t})}\\
...\\
\cfrac{P_{\theta}(S_{t+1} = N|\mathcal{Y}_{T} = \vec{y}_{T})}{P_{\theta}(S_{t+1} = N|\mathcal{Y}_{t} = \vec{y}_{t})}
\end{pmatrix}
\\ &amp;= P_{\theta}(S_{t} =j|\mathcal{Y}_{t} = \vec{y}_{t})\Pi_{j,}(\hat{\zeta}_{t+1|T} (\div) \hat{\zeta}_{t+1|t}).
\end{align*}\]</span> Where <span class="math inline">\(\Pi_{j,}\)</span> is the jth row of <span class="math inline">\(\Pi\)</span>. For the vector of probabilities one can therefore write: <span class="math display">\[\begin{align*}
    \hat{\zeta}_{t|T} = \hat{\zeta}_{t|t} \odot \Pi(\hat{\zeta}_{t+1|T} (\div) \hat{\zeta}_{t+1|t}).
\end{align*}\]</span> This is the earlier presented formula.</p>
</section>
<section id="em-algorithm-for-autoregressive-processes-with-finite-lag-order" class="level2">
<h2 class="anchored" data-anchor-id="em-algorithm-for-autoregressive-processes-with-finite-lag-order">EM Algorithm for Autoregressive Processes with finite lag order</h2>
<p>The here presented proofs for the equations (38), (39) and (40) closely follow Hamilton (1990, page 63-67). We begin with (38), then go on to (39) and finish with the proof for (40). The first, essential step for all three proofs, is to establish that the following holds: <span class="math display">\[\begin{equation}
\begin{aligned}
f_{\mathcal{Y}_{T:(m+1)},\mathcal{S}|\mathcal{Y}_m;\lambda}(\vec{y}_{T:(m+1)},\vec{s}_T|\vec{y}_m) = &amp; f_{Y_T|Z_T;\alpha}(y_T|z_T) \cdot P_{\Pi}(S_T = s_T|S_{T-1} = s_{T-1}) \\
&amp; \cdot f_{Y_{T-1}|Z_{T-1};\alpha}(y_{T-1}|z_{T-1}) \cdot P_{\Pi}(S_{T-1} = s_{T-1}|S_{T-2} = s_{T-2}) \\
&amp; \cdot... \\
&amp; \cdot f_{Y_{m+1}|Z_{m+1};\alpha}(y_{m+1}|z_{m+1}) \cdot P_{\Pi}(S_{m+1} = s_{m+1}|S_m = s_m) \\
&amp; \cdot \rho_{s_m,...,s_1}.
\end{aligned}
\end{equation}\]</span> This can be derived in the following way: <span class="math display">\[\begin{align*}
\displaystyle
&amp;f_{Y_T|Z_T;\alpha}(y_T|z_T) \cdot P_{\Pi}(S_T = s_T|S_{T-1} = s_{T-1})
\\&amp; \cdot f_{Y_{T-1}|Z_{T-1};\alpha}(y_{T-1}|z_{T-1}) \cdot P_{\Pi}(S_{T-1} = s_{T-1}|S_{T-2} = s_{T-2})\\
&amp; \cdot...\\
&amp; \cdot f_{Y_{m+1}|Z_{m+1};\alpha}(y_{m+1}|z_{m+1}) \cdot P_{\Pi}(S_{m+1} = s_{m+1}|S_m = s_m)\\
&amp; \cdot \rho_{s_m,...,s_1}
\\ &amp;= f_{Y_T|S_T,...,S_{T-m},Y_{T-1},...,Y_{T-m};\alpha}(y_T|s_T,...,s_{T-m},y_{T-1},...,y_{T-m})
P_{\Pi}(S_T = s_T|S_{T-1} = s_{T-1})\\
&amp; \hspace{0.5cm} \cdot f_{Y_{T-1}|S_{T-1},...,S_{T-1-m},Y_{T-1-1},...,Y_{T-1-m};\alpha}(y_{T-1}|s_{T-1},...,s_{T-1-m},y_{T-1-1},...,y_{T-1-m})
\\&amp; \hspace{0.5cm} \cdot P_{\Pi}(S_{T-1} = s_{T-1}|S_{T-2} = s_{T-2})
\\&amp; \hspace{0.5cm} \cdot \dots \\
&amp;  \hspace{0.5cm}\cdot f_{Y_{m+2}|S_{m+2},...,S_2,Y_{m+1},...,Y_2;\alpha}(y_{m+2}|s_{m+2},...,s_2,y_{m+1},...,y_2)
P_{\Pi}(S_{m+2} = s_{m+2}|S_{m+1} = s_{m+1})\\
&amp; \hspace{0.5cm}\cdot f_{Y_{m+1}|S_{m+1},...,S_1,Y_{m},...,Y_1;\alpha}(y_{m+1}|s_{m+1},...,s_1,y_{m},...,y_1)
P_{\Pi}(S_{m+1} = s_{m+1}|S_{m} = s_{m})\\
&amp; \hspace{0.5cm}\cdot P_{\lambda}(S_m = s_m,...,S_1 = s_1|Y_m = y_m,...,Y_1 = y_1).
\end{align*}\]</span> And due to the Markov property and (12) it holds that: <span class="math display">\[\begin{align*}
\displaystyle
&amp; f_{Y_{m+1}|S_{m+1},...,S_1,Y_{m},...,Y_1;\alpha}(y_{m+1}|s_{m+1},...,s_1,y_{m},...,y_1)
P_{\Pi}(S_{m+1} = s_{m+1}|S_{m} = s_{m})
\\ &amp;= f_{Y_{m+1}|S_{m+1},...,S_1,Y_{m},...,Y_1;\alpha}(y_{m+1}|s_{m+1},...,s_1,y_{m},...,y_1)
P_{\Pi}(S_{m+1} = s_{m+1}|S_{m} = s_{m},...,S_1 = s_1)
\\ &amp;= f_{Y_{m+1}|S_{m+1},...,S_1,Y_{m},...,Y_1;\alpha}(y_{m+1}|s_{m+1},...,s_1,y_{m},...,y_1)
\\&amp; \hspace{0.5cm} \cdot \ P_{\Pi}(S_{m+1} = s_{m+1}|S_{m} = s_{m},...,S_1 = s_1,Y_m = y_m,...,Y_1 = y_1)
\\ &amp;= f_{Y_{m+1},S_{m+1}|S_m,...,S_1,Y_m,...,Y_1;\theta}(y_{m+1},s_{m+1}|s_m,...,s_1,y_m,...,y_1).
\end{align*}\]</span> Logically it also holds that: <span class="math display">\[\begin{align*}
\displaystyle
&amp; f_{Y_{m+1},S_{m+1}|S_m,...,S_1,Y_m,...,Y_1;\theta}(y_{m+1},s_{m+1}|s_m,...,s_1,y_m,...,y_1)
\cdot P_{\lambda}(S_m = s_m,...,S_1 = s_1|Y_m = y_m,...,Y_1 = y_1)
\\&amp; = f_{Y_{m+1},S_{m+1},S_{m},...,S_1|Y_m,...,Y_1;\theta}(y_{m+1},s_{m+1},...,s_1|y_m,...,y_1).
\end{align*}\]</span> We assumed in our model formulation in 4.1 that there is a maximal autoregressive lag order <span class="math inline">\(m\)</span> such that <span class="math inline">\(Y_t\)</span>, depends only on <span class="math inline">\(m\)</span> lags of <span class="math inline">\(Y_t\)</span>. Then it holds that: <span class="math display">\[\begin{align*}
&amp;f_{Y_{m+2}|S_{m+2},...,S_2,Y_{m+1},...,Y_2;\alpha}(y_{m+2}|s_{m+2},...,s_2,y_{m+1},...,y_2)
P_{\Pi}(S_{m+2} = s_{m+2}|S_{m+1} = s_{m+1})
\\ &amp;= f_{Y_{m+2}|S_{m+2},...,S_2,S_1,Y_{m+1},...,Y_2,Y_1;\alpha}(y_{m+2}|s_{m+2},...,s_2,s_1,y_{m+1},...,y_2,y_1)
P_{\Pi}(S_{m+2} = s_{m+2}|S_{m+1} = s_{m+1})
\\ &amp;= f_{Y_{m+2}|S_{m+2},...,S_2,S_1,Y_{m+1},...,Y_2,Y_1;\alpha}(y_{m+2}|s_{m+2},...,s_2,s_1,y_{m+1},...,y_2,y_1)
\\&amp;  \hspace{0.5cm} \cdot P_{\Pi}(S_{m+2} = s_{m+2}|S_{m+1} = s_{m+1},S_m = s_m,...,S_1 = s_1,Y_{m+1} = y_{m+1},...,Y_1 = y_1)
\\ &amp;= f_{Y_{m+2},S_{m+2}|S_{m+1},S_m,...,S_1,Y_{m+1},Y_m,...,Y_1;\theta}(y_{m+2},s_{m+2}|s_{m+1},s_m,...,s_1,y_{m+1},y_m,...,y_1).
\end{align*}\]</span> And thus we can write: <span class="math display">\[\begin{align*}
\displaystyle
&amp;f_{Y_{m+2},S_{m+2}|S_{m+1},S_m,...,S_1,Y_{m+1},Y_m,...,Y_1;\theta}(y_{m+2},s_{m+2}|s_{m+1},s_m,...,s_1, y_{m+1},y_m,...,y_1)
\\ &amp;\cdot f_{Y_{m+1},S_{m+1},S_{m},...,S_1|Y_m,...Y_1;\theta}(y_{m+1},s_{m+1},s_{m},...,s_1|y_m,...,y_1)
\\ &amp;= f_{Y_{m+2},S_{m+2}|S_{m+1},Y_{m+1},S_m,...,S_1,Y_m,...,Y_1;\theta}(y_{m+2},s_{m+2}|s_{m+1},y_{m+1},s_m,...,s_1,y_m,...,y_1)
\\&amp; \hspace{0.5cm} \cdot f_{Y_{m+1},S_{m+1},S_{m},...,S_1|Y_m,...Y_1;\theta}(y_{m+1},s_{m+1},s_{m},...,s_1|y_m,...,y_1)
\\ &amp;= f_{Y_{m+2},S_{m+2},Y_{m+1},S_{m+1},S_m,...S_1|Y_{m},...Y_1;\theta}(y_{m+2},s_{m+2},y_{m+1},s_{m+1}, s_m,...,s_1|y_{m},...,y_1).
\end{align*}\]</span> We can follow this logic until <span class="math inline">\(T\)</span> and end up with: <span class="math display">\[\begin{align*}
\displaystyle
&amp; f_{Y_T|S_T,...,S_{T-m},Y_{T-1},...,Y_{T-m};\alpha}(y_T|s_T,...,s_{T-m},y_{T-1},...,y_{T-m})P_{\Pi}(S_T = s_T|S_{T-1} = s_{T-1})\\
&amp;\cdot f_{Y_{T-1}|S_{T-1},...,S_{T-1-m},Y_{T-1-1},...,Y_{T-1-m};\alpha}(y_{T-1}|s_{T-1},...,s_{T-1-m}, y_{T-1-1},...,y_{T-1-m})
\\&amp; \cdot P_{\Pi}(S_{T-1} = s_{T-1}|S_{T-2} = s_{T-2})\\
&amp; \cdot \dots\\
&amp;\cdot f_{Y_{m+2}|S_{m+2},...,S_2,Y_{m+1},...,Y_2;\alpha}(y_{m+2}|s_{m+2},...,s_2, y_{m+1},...,y_2)P_{\Pi}(S_{m+2} = s_{m+2}|S_{m+1} = s_{m+1})\\
&amp;\cdot f_{Y_{m+1}|S_{m+1},...,S_2,Y_{m},...,Y_1;\alpha}(y_{m+1}|s_{m+1},...,s_2,y_{m},...,y_1) P_{\Pi}(S_{m+1} = s_{m+1}|S_m = s_{m})\\
&amp;\cdot P_{\lambda}(S_m = s_m,...,S_1 = s_1|Y_m = y_m,...,Y_1 = y_1)
\\ &amp;= f_{Y_T,S_T,Y_{T-1},S_{T-1},...,Y_{m+1},S_{m+1},S_m,...,S_1|Y_m,...,Y_1;\lambda}(y_T,s_T,y_{T-1}, s_{T-1},...,y_{m+1},s_{m+1}, s_m,..., s_1| y_m,..., y_1)
\\ &amp;= f_{\mathcal{Y}_{T:(m+1)},\mathcal{S}|\mathcal{Y}_m;\lambda}(\vec{y}_{T:(m+1)},\vec{s}_T|\vec{y}_m).
\end{align*}\]</span></p>
Now that this first step has been established, we can now focus on the derivation of the first equation, thereby we are closely following Hamilton (1990, page 63-65). We start with (55), it holds that: <span class="math display">\[\begin{equation}
\begin{aligned}
\ln(f_{\mathcal{Y}_{T:(m+1)},\mathcal{S}|\mathcal{Y}_m;\lambda}(\vec{y}_{T:(m+1)},\vec{s}_T|\vec{y}_m))
&amp;= \ln(f_{Y_T|Z_T;\alpha}(y_T|z_T))
\\&amp; \hspace{0.5cm} + \ln(P_{\Pi}(S_T = s_T|S_{T-1} = s_{T-1}))\\
&amp; \hspace{0.5cm}+ \ln(f_{Y_{T-1}|Z_{T-1};\alpha}(y_{T-1}|z_{T-1}))
\\&amp; \hspace{0.5cm} + \ln(P_{\Pi}(S_{T_1} = s_{T-1}|S_{T-2} = s_{T-2}))\\
&amp;\hspace{0.5cm} + \dots \\
&amp;\hspace{0.5cm} + \ln(f_{Y_{m+1}|Z_{m+1};\alpha}(y_{m+1}|z_{m+1}))
\\&amp;\hspace{0.5cm} + \ln(P_{\Pi}(S_{m+1} = s_{m+1}|S_m = s_m))\\
&amp;\hspace{0.5cm} + \ln(\rho_{s_m,\dots,s_1}).
\end{aligned}
\end{equation}\]</span> We remember that if we have <span class="math inline">\(L(x,y)\)</span> and <span class="math inline">\(\ln(L(x,y)) = l(x,y)\)</span>, then it is true that: <span class="math display">\[\begin{align*}
\cfrac{\partial l(x,y)}{\partial x} = \cfrac{1}{L(x,y)}\cfrac{\partial L(x,y)}{\partial x},
\end{align*}\]</span> and thus <span class="math display">\[\begin{align*}
\cfrac{\partial L(x,y)}{\partial x} = \cfrac{\partial l(x,y)}{\partial x}L(x,y).
\end{align*}\]</span> We apply this now: <span class="math display">\[\begin{align*}
\displaystyle
&amp;f_{\mathcal{Y}_{T:(m+1)},\mathcal{S}|\mathcal{Y}_m;\lambda}(\vec{y}_{T:(m+1)},\vec{s}_T|\vec{y}_m) \cfrac{\partial \ln(f_{\mathcal{Y}_{T:(m+1)},\mathcal{S}|\mathcal{Y}_m;\lambda}(\vec{y}_{T:(m+1)},\vec{s}_T|\vec{y}_m))}{\partial \pi_{i,j}} \\ &amp;= \cfrac{\partial f_{\mathcal{Y}_{T:(m+1)},\mathcal{S}|\mathcal{Y}_m;\lambda}(\vec{y}_{T:(m+1)},\vec{s}_T|\vec{y}_m)}{\partial \pi_{i,j}},
\end{align*}\]</span> where: <span class="math display">\[\begin{align*}
\displaystyle
\cfrac{\partial \ln(f_{\mathcal{Y}_{T:(m+1)},\mathcal{S}|\mathcal{Y}_m;\lambda}(\vec{y}_{T:(m+1)},\vec{s}_T|\vec{y}_m))}{\partial \pi_{i,j}} = \sum_{t = m+1}^{T}\cfrac{\partial \ln(P_{\Pi}(S_t = s_t|S_{t-1} = s_{t-1}))}{\partial \pi_{i,j}}.
\end{align*}\]</span> One should note that: <span class="math display">\[\begin{align*}
\displaystyle
\cfrac{\partial \ln(P_{\Pi}(S_t = s_t|S_{t-1} = s_{t-1}))}{\partial \pi_{i,j}} =
\begin{cases}
    \cfrac{1}{\pi_{i,j}}, &amp; \text{if $S_t =j \quad \text{and} \quad S_{t-1} = i$} \\
    0, &amp; \text{otherwise}
\end{cases}.
\end{align*}\]</span> In the following, we will use the Kronecker delta as notation in the following way: <span class="math display">\[\begin{align*}
\delta_{[A]} =
\begin{cases}
    1, &amp;\text{if A is true}\\
    0, &amp;\text{otherwise}
\end{cases},
\end{align*}\]</span> thus: <span class="math display">\[\begin{align*}
\displaystyle
\cfrac{\partial f_{\mathcal{Y}_{T:(m+1)},\mathcal{S}|\mathcal{Y}_m;\lambda}(\vec{y}_{T:(m+1)},\vec{s}_T|\vec{y}_m)}{\partial \pi_{i,j}} &amp;= f_{\mathcal{Y}_{T:(m+1)},\mathcal{S}|\mathcal{Y}_m;\lambda}(\vec{y}_{T:(m+1)},\vec{s}_T|\vec{y}_m)\sum_{t = m+1}^{T}\cfrac{\partial \ln(P_{\Pi}(S_t = s_t|S_{t-1} = s_{t-1}))}{\partial \pi_{i,j}}
\\ &amp;= f_{\mathcal{Y}_{T:(m+1)},\mathcal{S}|\mathcal{Y}_m;\lambda}(\vec{y}_{T:(m+1)},\vec{s}_T|\vec{y}_m)\cfrac{1}{\pi_{i,j}}\sum_{t = m+1}^{T}\delta_{[S_t =j,S_{t-1}= i]}.
\end{align*}\]</span> We remember that the following holds: <span class="math display">\[\begin{align*}
\displaystyle
    Q_{\lambda_l,\vec{y}_{T}}(\lambda_{l+1}) = \sum_{\vec{s}_{T}} \ln(f_{\mathcal{Y}_{T:(m+1)},\mathcal{S}|\mathcal{Y}_m;\lambda_{l+1}}(\vec{y}_{T:(m+1)},\vec{s}_T|\vec{y}_m))f_{\mathcal{Y}_{T:(m+1)},\mathcal{S}|\mathcal{Y}_m;\lambda_{l}}(\vec{y}_{T:(m+1)},\vec{s}_T|\vec{y}_m).
\end{align*}\]</span> Therefore, we can say: <span class="math display">\[\begin{align*}
\displaystyle
\cfrac{\partial Q_{\lambda_l,\vec{y}_{T}}(\lambda_{l+1})}{\partial \pi_{i,j}^{(l+1)}} &amp;= \sum_{\vec{s}_{T}}\cfrac{\partial \ln(f_{\mathcal{Y}_{T:(m+1)},\mathcal{S}|\mathcal{Y}_m;\lambda_{l+1}}(\vec{y}_{T:(m+1)},\vec{s}_T|\vec{y}_m))}{\partial \pi_{i,j}^{(l+1)}}f_{\mathcal{Y}_{T:(m+1)},\mathcal{S}|\mathcal{Y}_m;\lambda_{l}}(\vec{y}_{T:(m+1)},\vec{s}_T|\vec{y}_m)
\\ &amp;= \sum_{\vec{s}_{T}}\cfrac{1}{\pi_{i,j}^{(l+1)}}\sum_{t = m+1}^{T}\delta_{[S_t = j, S_{t-1} = i]}f_{\mathcal{Y}_{T:(m+1)},\mathcal{S}|\mathcal{Y}_m;\lambda_l}(\vec{y}_{T:(m+1)},\vec{s}_T|\vec{y}_m).
\end{align*}\]</span> It is now essential to notice that: <span class="math display">\[\begin{align*}
\displaystyle
\sum_{\vec{s}_{T}}\delta_{[S_t = j,S_{t-1} = i]}f_{\mathcal{Y}_{T:(m+1)},\mathcal{S}|\mathcal{Y}_m;\lambda_{l}}(\vec{y}_{T:(m+1)},\vec{s}_T|\vec{y}_m)
&amp;= f_{\mathcal{Y}_{T:(m+1)},S_t, S_{t-1}|\mathcal{Y}_m;\lambda_l}(\vec{y}_{T:(m+1)},j,i)
\\ &amp;= P_{\lambda_l}(S_t = j, S_{t-1} = i|\mathcal{Y}_T = \vec{y}_{T})
\\&amp; \hspace{0.5cm} \cdot f_{\mathcal{Y}_{T:(m+1)}|\mathcal{Y}_m;\lambda_{l}}(\vec{y}_{T:(m+1)}|\vec{y}_m),
\end{align*}\]</span> and that therefore: <span class="math display">\[\begin{align*}
\displaystyle
\cfrac{\partial Q_{\lambda_l,\vec{y}_{T}}(\lambda_{l+1})}{\partial \pi_{i,j}^{(l+1)}}
&amp;= \sum_{\vec{s}_{T}}\cfrac{1}{\pi_{i,j}^{(l+1)}}\sum_{t = m+1}^{T}\delta_{[S_t = j, S_{t-1} = i]} f_{\mathcal{Y}_{T:(m+1)},\mathcal{S}|\mathcal{Y}_m;\lambda_l}(\vec{y}_{T:(m+1)},\vec{s}_T|\vec{y}_m)
\\ &amp;=\cfrac{1}{\pi_{i,j}^{(l+1)}}\sum_{t = m+1}^{T}\sum_{\vec{s}_{T}}\delta_{[S_t = j, S_{t-1} = i]}f_{\mathcal{Y}_{T:(m+1)},\mathcal{S}|\mathcal{Y}_m;\lambda_l}(\vec{y}_{T:(m+1)},\vec{s}_T|\vec{y}_m)
\\ &amp;= \cfrac{1}{\pi_{i,j}^{(l+1)}}\sum_{t = m+1}^{T}P_{\lambda_l}(S_t = j,S_{t-1} = i|\mathcal{Y}_T = \vec{y}_{T})f_{\mathcal{Y}_{T:(m+1)}|\mathcal{Y}_m;\lambda_{l}}(\vec{y}_{T:(m+1)}|\vec{y}_m).
\end{align*}\]</span> Under the constraint <span class="math inline">\(\sum_{j = 1}^{N}\pi_{i,j} = 1\)</span> we can now form the Lagrangian: <span class="math display">\[\begin{align*}
\displaystyle
Q_{\lambda_l,\vec{y}_{T}}(\lambda_{l+1}) - \mu_i(\sum_{j = 1}^{N}\pi_{i,j} - 1).
\end{align*}\]</span> This leads to the following first-order conditons: <span class="math display">\[\begin{align*}
\cfrac{\partial Q_{\lambda_{l},\vec{y}_{T}}(\lambda_{l+1})}{\partial \pi_{i,j}^{(l+1)}} = \mu_i, \quad \text{for} \quad j = 1,...,N.
\end{align*}\]</span> We insert our result from above: <span class="math display">\[\begin{align*}
\displaystyle
&amp; \cfrac{1}{\pi_{i,j}^{(l+1)}}\sum_{t = m+1}^{T}P_{\lambda_l}(S_t = j, S_{t-1} = i|\mathcal{Y}_T = \vec{y}_{T})f_{\mathcal{Y}_{T:(m+1)}|\mathcal{Y}_m;\lambda_{l}}(\vec{y}_{T:(m+1)}|\vec{y}_m) = \mu_i
\\ &amp;\Leftrightarrow \sum_{t = m+1}^{T}P_{\lambda_l}(S_t =j,S_{t-1}=i|\mathcal{Y}_T = \vec{y}_{T}) = \cfrac{\pi_{i,j}^{(l+1)}\mu_i}{f_{\mathcal{Y}_{T:(m+1)}|\mathcal{Y}_m;\lambda_{l}}(\vec{y}_{T:(m+1)}|\vec{y}_m)}.
\end{align*}\]</span> We now sum over <span class="math inline">\(1,...,N\)</span>, which leads to: <span class="math display">\[\begin{align*}
\displaystyle
&amp;\sum_{j = 1}^{N}\sum_{t = m+1}^{T}P_{\lambda_l}(S_t = j,S_{t-1} = i|\mathcal{Y}_T = \vec{y}_{T}) = \sum_{j = 1}^{N}\cfrac{\pi_{i,j}^{(l+1)}\mu_i}{f_{\mathcal{Y}_{T:(m+1)}|\mathcal{Y}_m;\lambda_{l}}(\vec{y}_{T:(m+1)}|\vec{y}_m)}
\\ &amp;\Leftrightarrow \sum_{t = m+1}^{T}P_{\lambda_l}(S_{t-1} = i|\mathcal{Y}_T = \vec{y}_{T}) = \cfrac{\mu_i}{f_{\mathcal{Y}_{T:(m+1)}|\mathcal{Y}_m;\lambda_{l}}(\vec{y}_{T:(m+1)}|\vec{y}_m)}.
\end{align*}\]</span> If we now insert this in the result from above we get: <span class="math display">\[\begin{align*}
&amp;\sum_{t = m+1}^{T}P_{\lambda_l}(S_t =j,S_{t-1}=i|\mathcal{Y}_T = \vec{y}_{T}) = \cfrac{\pi_{i,j}^{(l+1)}\mu_i}{f_{\mathcal{Y}_{T:(m+1)}|\mathcal{Y}_m;\lambda_{l}}(\vec{y}_{T:(m+1)}|\vec{y}_m)} = \pi_{i,j}^{(l+1)}\sum_{t = m+1}^{T}P_{\lambda_l}(S_{t-1} = i|\mathcal{Y}_T = \vec{y}_{T})
\\ &amp;\Leftrightarrow \pi_{i,j}^{(l+1)} = \cfrac{\sum_{t = m+1}^{T}P_{\lambda_l}(S_t =j,S_{t-1}=i|\mathcal{Y}_T = \vec{y}_{T})}{\sum_{t = m+1}^{T}P_{\lambda_l}(S_{t-1} = i|\mathcal{Y}_T  =\vec{y}_{T})},
\end{align*}\]</span> this concludes the derivation of (38).
With that, we get to the second equation. In the following we closely follow Hamilton (1990, page 65-66). Again, we start with (55), but this time we take the derivative in <span class="math inline">\(\alpha\)</span>: <span class="math display">\[\begin{align*}
\displaystyle
\cfrac{\partial f_{\mathcal{Y}_{T:(m+1)},\mathcal{S}|\mathcal{Y}_m;\lambda}(\vec{y}_{T:(m+1)},\vec{s}_T|\vec{y}_m)}{\partial \alpha} = f_{\mathcal{Y}_{T:(m+1)},\mathcal{S}|\mathcal{Y}_m;\lambda}(\vec{y}_{T:(m+1)},\vec{s}_T|\vec{y}_m)\sum_{t = m+1}^{T}\cfrac{\partial \ln(f_{Y_t|Z_t;\alpha}(y_t|z_t))}{\partial \alpha}.
\end{align*}\]</span> It is important to note here that <span class="math inline">\(f_{Y_t|Z_t;\alpha}(y_t|z_t)\)</span> depends on <span class="math inline">\(S_t\)</span> through <span class="math inline">\(Z_t\)</span>, because \ <span class="math inline">\(Z_t = (S_t,S_{t-1},...,S_{t-m},Y_{t-1},Y_{t-2},...,Y_{t-m})\)</span>, but at most for the dates <span class="math inline">\(t,...,t-m\)</span>, thus: <span class="math display">\[\begin{align*}
\displaystyle
\cfrac{\partial Q_{\lambda_l,\vec{y}_{T}}(\lambda_{l+1})}{\partial \alpha_{l+1}} &amp;= \cfrac{\partial}{\partial \alpha_{l+1}}\sum_{\vec{s}_{T}}\ln(f_{\mathcal{Y}_{T:(m+1)},\mathcal{S}|\mathcal{Y}_m;\lambda_{l+1}}(\vec{y}_{T:(m+1)},\vec{s}_T|\vec{y}_m))f_{\mathcal{Y}_{T:(m+1)},\mathcal{S}|\mathcal{Y}_m;\lambda_{l}}(\vec{y}_{T:(m+1)},\vec{s}_T|\vec{y}_m)
\\ &amp;= \sum_{\vec{s}_{T}}\cfrac{\partial \ln(f_{\mathcal{Y}_{T:(m+1)},\mathcal{S}|\mathcal{Y}_m;\lambda_{l+1}}(\vec{y}_{T:(m+1)},\vec{s}_T|\vec{y}_m))}{\partial \alpha_{l+1}}f_{\mathcal{Y}_{T:(m+1)},\mathcal{S}|\mathcal{Y}_m;\lambda_{l}}(\vec{y}_{T:(m+1)},\vec{s}_T|\vec{y}_m)
\\ &amp; \stackrel{\text{(56)}}{=}\sum_{\vec{s}_{T}}f_{\mathcal{Y}_{T:(m+1)},\mathcal{S}|\mathcal{Y}_m;\lambda_{l}}(\vec{y}_{T:(m+1)},\vec{s}_T|\vec{y}_m)\sum_{t = m+1}^{T}\cfrac{\partial \ln(f_{Y_t|Z_t;\alpha_{l+1}}(y_t|z_t))}{\partial \alpha_{l+1}}
\\ &amp;= \sum_{t = m + 1}^{T}\sum_{\vec{s}_T}\cfrac{\partial \ln(f_{Y_t|Z_t;\alpha_{l+1}}(y_t|z_t))}{\partial \alpha_{l+1}}f_{\mathcal{Y}_{T:(m+1)},\mathcal{S}|\mathcal{Y}_m;\lambda_{l}}(\vec{y}_{T:(m+1)},\vec{s}_T|\vec{y}_m)
\\ &amp;= \sum_{t = m+1}^{T}\sum_{s_t = 1}^{N}\cdots\sum_{s_{t-m} = 1}^{N}\cfrac{\partial \ln(f_{Y_t|Z_t;\alpha_{l+1}}(y_t|z_t))}{\partial \alpha_{l+1}}
\\&amp; \hspace{0.5cm}\cdot \left(\sum_{s_{T} = 1}^{N}\cdots\sum_{s_{t+1} = 1}^{N}\sum_{s_{t-m-1} = 1}^{N}\cdots\sum_{s_{1} = 1}^{N}f_{\mathcal{Y}_{T:(m+1)},\mathcal{S}|\mathcal{Y}_m;\lambda_{l}}(\vec{y}_{T:(m+1)},\vec{s}_T|\vec{y}_m)\right)
\\ &amp;= \sum_{t = m+1}^{T}\sum_{s_t = 1}^{N}\cdots\sum_{s_{t-m} = 1}^{N}\cfrac{\partial \ln(f_{Y_t|Z_t;\alpha_{l+1}}(y_t|z_t))}{\partial \alpha_{l+1}}
\\ &amp; \hspace{0.5cm} \cdot P_{\lambda_l}(S_t = s_t,...,S_{t-m} = s_{t-m}|Y_T = y_T,...,Y_1 = y_1)
\\&amp; \hspace{0.5cm} \cdot f_{Y_T,...,Y_{m+1}|Y_m,...,Y_1;\lambda_l}(y_T,...,y_{m+1}|y_m,...,y_1).
\end{align*}\]</span> These steps are possible because <span class="math inline">\(f_{Y_t|Z_t;\alpha}(y_t|z_t)\)</span> at most only depends on <span class="math inline">\(S_t,...,S_{t-m}\)</span>. This leads us to the following first order condition: <span class="math display">\[\begin{align*}
\displaystyle
&amp;f_{\mathcal{Y}_{T:(m+1)}|\mathcal{Y}_m;\lambda_{l}}(\vec{y}_{T:(m+1)}|\vec{y}_m)\sum_{t = m+1}^{T}\sum_{s_t = 1}^{N}\cdots\sum_{s_{t-m} = 1}^{N} \cfrac{\partial \ln(f_{Y_t|Z_t;\alpha_{l+1}}(y_t|z_t))}{\partial \alpha_{l+1}}
\\&amp; \cdot P_{\lambda_l}(S_t = s_t,S_{t-1} = s_{t-1},...,S_{t-m} = s_{t-m}|\mathcal{Y}_T = \vec{y}_{T}) = 0,
\end{align*}\]</span> which is equivalent to (39).
<p>Now we can turn to equation number three, here we closely follow the derivations presented by Hamilton (1990, page 66-67). We start with (56) and take the derivative in <span class="math inline">\(\rho_{i_m,...,i_1}\)</span>: <span class="math display">\[\begin{align*}
\displaystyle
\cfrac{\partial \ln(f_{\mathcal{Y}_{T:(m+1)},\mathcal{S}|\mathcal{Y}_m;\lambda_{l}}(\vec{y}_{T:(m+1)},\vec{s}_T|\vec{y}_m))}{\partial \rho_{i_m,...,i_1}} = \cfrac{1}{\rho_{i_m,...,i_1}}\cdot\delta_{[S_m = i_m,...,S_1 = i_1]},
\end{align*}\]</span> because of this, it holds that:\ <span class="math display">\[\begin{align*}
\displaystyle
\cfrac{\partial Q_{\lambda_l,\vec{y}_{T}}(\lambda_{l+1})}{\partial\rho_{i_m,...,i_1}^{(l+1)}}
&amp;= \sum_{\vec{s}_T}\cfrac{\partial \ln(f_{\mathcal{Y}_{T:(m+1)},\mathcal{S}|\mathcal{Y}_m;\lambda_{l+1}}(\vec{y}_{T:(m+1)},\vec{s}_T|\vec{y}_m))}{\partial \rho_{i_m,...,i_1}^{(l+1)}}f_{\mathcal{Y}_{T:(m+1)},\mathcal{S}|\mathcal{Y}_m;\lambda_{l}}(\vec{y}_{T:(m+1)},\vec{s}_T|\vec{y}_m)
\\ &amp;= \sum_{\vec{s}_{T}}\cfrac{1}{\rho_{i_m,...,i_1}^{(l+1)}}\delta_{[S_m = i_m,...,S_1 = i_1]}f_{\mathcal{Y}_{T:(m+1)},\mathcal{S}|\mathcal{Y}_m;\lambda_{l}}(\vec{y}_{T:(m+1)},\vec{s}_T|\vec{y}_m).
\end{align*}\]</span> We want to optimize <span class="math inline">\(Q_{\lambda_l,\vec{y}_{T}}(\lambda_{l+1})\)</span> under the constraint <span class="math inline">\(\sum_{j = 1}^{N^m}(\rho_{l+1})_j = 1\)</span>, i.e that the sum of all elements of <span class="math inline">\(\rho_{l+1}\)</span> shall be 1. Thus we construct the Lagrangian: <span class="math display">\[\begin{align*}
\displaystyle
Q_{\lambda_l,\vec{y}_{T}}(\lambda_{l+1}) - \mu(\sum_{j = 1}^{N^m}(\rho_{l+1})_j-1)).
\end{align*}\]</span> Which leads to the first order condition: <span class="math display">\[\begin{align*}
\displaystyle
&amp;\sum_{\vec{s}_{T}} \cfrac{1}{\rho_{i_m,...,i_1}^{(l+1)}}\delta_{[S_m = i_m,...,S_1 = i_1]}f_{\mathcal{Y}_{T:(m+1)},\mathcal{S}|\mathcal{Y}_m;\lambda_{l}}(\vec{y}_{T:(m+1)},\vec{s}_T|\vec{y}_m) = \mu
\\ &amp;\Leftrightarrow \sum_{\vec{s}_T}\delta_{[S_m = i_m,...,S_1 = i_1]}f_{\mathcal{Y}_{T:(m+1)},\mathcal{S}|\mathcal{Y}_m;\lambda_{l}}(\vec{y}_{T:(m+1)},\vec{s}_T|\vec{y}_m) = \rho_{i_m,...,i_1}^{(l+1)}\mu
\\ &amp;\Leftrightarrow f_{S_m,...,S_1,Y_T,...,Y_{m+1}|Y_m,...,Y_1;\lambda_l}(i_m,...,i_1,y_T,...,y_{m+1}|y_m,...,y_1) = \rho_{i_m,...,i_1}^{(l+1)}\mu
\\ &amp;\Leftrightarrow P_{\lambda_l}(S_m = i_m,...,S_1 = i_1|\mathcal{Y}_T  =\vec{y}_{T})f_{\mathcal{Y}_{T:(m+1)}|\mathcal{Y}_m;\lambda_{l}}(\vec{y}_{T:(m+1)}|\vec{y}_m) = \mu \rho_{i_m,...,i_1}^{(l+1)}.
\end{align*}\]</span> If we now sum over all potential values of <span class="math inline">\((i_1,...i_m)\)</span> we end up with: <span class="math display">\[\begin{align*}
\displaystyle
&amp;\sum_{i_m = 1}^{N}\cdots\sum_{i_1 = 1}^{N}P_{\lambda_l}(S_m = i_m,...,S_1 = i_1|\mathcal{Y}_T = \vec{y}_{T})f_{\mathcal{Y}_{T:(m+1)}|\mathcal{Y}_m;\lambda_{l}}(\vec{y}_{T:(m+1)}|\vec{y}_m)
= \sum_{i_m = 1}^{N}\cdots\sum_{i_1 = 1}^{N}\mu \rho_{i_m,...,i_1}^{(l+1)}
\\ &amp;\Leftrightarrow f_{\mathcal{Y}_{T:(m+1)}|\mathcal{Y}_m;\lambda_{l}}(\vec{y}_{T:(m+1)}|\vec{y}_m) = \mu.
\end{align*}\]</span> We insert this for <span class="math inline">\(\mu\)</span> and get: <span class="math display">\[\begin{align*}
\displaystyle
&amp;P_{\lambda_l}(S_m = i_m,...,S_1 = i_1|\mathcal{Y}_T = \vec{y}_{T})f_{\mathcal{Y}_{T:(m+1)}|\mathcal{Y}_m;\lambda_{l}}(\vec{y}_{T:(m+1)}|\vec{y}_m) = f_{\mathcal{Y}_{T:(m+1)}|\mathcal{Y}_m;\lambda_{l}}(\vec{y}_{T:(m+1)}|\vec{y}_m)\rho_{i_m,...,i_1}^{(l+1)}
\\ &amp;\Leftrightarrow P_{\lambda_l}(S_m = i_m,...,S_1 = i_1|\mathcal{Y}_T = \vec{y}_{T}) = \rho_{i_m,...,i_1}^{(l+1)}.
\end{align*}\]</span> This concludes the derivation of the EM algorithm for models with an underlying Markov-Chain and a dependence on a maximum lag order.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>